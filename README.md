<div align="center">
  <img src="assets/logo.png" alt="infiAgent Logo" width="200">

  <h1>MLA V3 - Build Domain-Specific SOTA-Level AI Agents</h1>

  <p>
    <img src="https://img.shields.io/badge/version-3.0.0-blue.svg" alt="Version">
    <img src="https://img.shields.io/badge/python-3.9+-green.svg" alt="Python">
    <img src="https://img.shields.io/badge/license-GPL-blue.svg" alt="License: GPL">
  </p>

  <p>
    <a href="README.md">English</a> | <a href="README_CN.md">ç®€ä½“ä¸­æ–‡</a>
  </p>
</div>

---

## ğŸŒŸ Introduction

**infiAgent Also called MLA (Multi-Level Agent)** is an agent framework designed for **unlimited runtime** without tool calling chaos or system crashes caused by cumulative task resources and conversation history. With MLA, you can build powerful general-purpose and semi-specialized agents simply by writing configuration files.

### Key Features

- âœ… **Unlimited Runtime**: No degradation from context accumulation
- âœ… **Multi-Level Agent Hierarchy**: Serial execution with tree-structured agent orchestration
- âœ… **Zero Context Compression**: File-based state management eliminates the need for context compression
- âœ… **Task Memory**: Persistent memory across sessions using workspace as task ID
- âœ… **Complete Research Workflows**: From literature search to experiments, plotting, and LaTeX papers

### Default Configuration

The default configuration in this repository is a **research-oriented semi-specialized agent** capable of:

- ğŸ“ **Academic Paper Writing**: Complete end-to-end workflow from research to LaTeX submission
- âœ… **Human-Level Quality**: Papers can pass EI/IEEE conference peer reviews
- ğŸ§ª **Scientific Computing**: ECM protein simulation, logistics scheduling, assignment grading, etc.
- ğŸ”¬ **Full Research Pipeline**: Literature collection, experiments, figures, and paper drafting

### Update

If you pulled the image or code before the latest update date, please refer to the issues that have been fixed and, based on your needs, pull the image and code again.

- [2026/01/06] Web UI: add an entry-agent selector next to Task ID so you can choose the root agent for the conversation, with an agent list and a visual agent tree for the selected root.

- [2026/01/05] Resolves global freeze caused by prolonged unresponsiveness of the primary token. Please update code or pull latest docker image!

- [2026/01/04] Support different Language of Agent output base on user input.

- [2026/01/03] Optimize LiteLLMâ€™s native retry mechanism by enhancing error-aware retry prompts to improve small-model call success rates; add connection timeout detection to reduce task interruption risks.

- [2026/01/02] Install and how use vedio please click <a href="https://www.bilibili.com/video/BV142vQB2EDu/?share_source=copy_web&vd_source=8f099df5d40b73e936381e57d7fc05aa
">infiagent:å…¨è‡ªåŠ¨å†™ä½œå·¥å…·</a>

- [2026/01/02] fix some bugs about reference manage, Please clone latest repo or pull latest docker image: chenglinhku/mlav3.

- [2026/01/01] support web_ui and qwen api. Also fix some problem when using third part oepnai format api. please using latest chenglinhku/mlav3 docker image and see the example configs.

- [2025/12/31] support gemini api key from google ai studio now. Please See the gemini config in dir. 


Attention: Current coding task only support python project. Other language may supported later. In old version execute_command only support safe command like cd or grepï¼Œnow it include every commands including rm. Please try to use it in docker mode if your task may edit system file.

## ğŸ¬ Outputs

complete academic papers generated by MLA:

**Demo 1:**

<p align="center">
  <img src="assets/paper_generation_demo_1.gif" alt="Paper Generation Demo 1" width="800">
</p>

**Demo 2:**

<p align="center">
  <img src="assets/paper1.png" alt="Paper Generation Demo 2" width="800">
</p>

**Demo 3:**

<p align="center">
  <img src="assets/paper2.png" alt="Paper Generation Demo 3" width="800">
</p>

MLA handles the entire research workflow - from literature search and experiment design to code execution, figure generation, and LaTeX paper writing. All automatically orchestrated through multi-level agents.

---

## ğŸ“š Table of Contents

- [See It In Action](#-see-it-in-action)
- [Quick Start](#-quick-start)
- [How It Works](#-how-it-works)
- [Interface Screenshots](#-interface-screenshots)
- [Configuration Guide](#-configuration-guide)
- [CLI Interface](#-cli-interface)
- [SDK Integration](#-sdk-integration)
- [Example Outputs](#-example-outputs)

---

## ğŸš€ Quick Start

### Vedio of Docker Mode:

<a href="https://www.bilibili.com/video/BV142vQB2EDu/?share_source=copy_web&vd_source=8f099df5d40b73e936381e57d7fc05aa
">infiagent:å…¨è‡ªåŠ¨å†™ä½œå·¥å…·</a>

### Option 1: Docker (Recommended - No Python Required)

**1. Install Docker**
- Mac/Windows: [Docker Desktop](https://www.docker.com/products/docker-desktop)
- Linux: `curl -fsSL https://get.docker.com | sh`

**2. Pull Image**

```bash
docker pull chenglinhku/mlav3:latest
```

**3. Choose Your Mode**

### Option A: Web UI Mode (Recommended)
open localhost:9641 to set keys and base url.

```bash
cd /your/workspace
# XXXX is optional port for agent web development (replace with your port like 5002)
docker run -d --name mla \
  -e HOST_PWD=$(pwd) \
  -v $(pwd):/workspace$(pwd) \
  -v ~/.mla_v3:/root/mla_v3 \
  -v mla-config:/mla_config \
  -p 8002:8002 \
  -p 9641:9641 \
  -p 4242:4242 \
  -p 5002:5002 \
  chenglinhku/mlav3:latest webui && docker logs -f mla
```

Then open browser: `http://localhost:4242`
default usernameï¼šuser defaultpasswordï¼špassword

<p align="center">
  <img src="assets/web_ui.png" alt="Paper Generation Demo 2" width="800">
</p>

ğŸ“– **Web UI usage & UI details**: see [web_ui/README.md](web_ui/README.md).

### Option B: CLI Mode

```bash
cd /your/workspace
# XXXX is optional port for agent web development (replace with your port like 5002)
docker run -it --rm \
  -e HOST_PWD=$(pwd) \
  -v $(pwd):/workspace$(pwd) \
  -v ~/.mla_v3:/root/mla_v3 \
  -v mla-config:/mla_config \
  -p 8002:8002 \
  -p 9641:9641 \
  -p 5002:5002 \
  chenglinhku/mlav3:latest cli
```

**Windows Users:**

Windows users need to manage conversation IDs manually. Different task IDs maintain different memories.

```powershell
# CLI Mode (PowerShell)
docker run -it --rm `
  -e HOST_PWD="/{your_conversation_id}" `
  -v "${PWD}:/workspace/{your_conversation_id}" `
  -v "${HOME}\.mla_v3:/root/mla_v3" `
  -v mla-config:/mla_config `
  -p 8002:8002 `
  -p 9641:9641 `
  -p 5002:5002 `
  chenglinhku/mlav3:latest cli

# Web UI Mode (PowerShell)
docker run -d --name mla-webui `
  -e HOST_PWD="/{your_conversation_id}" `
  -v "${PWD}:/workspace/{your_conversation_id}" `
  -v "${HOME}\.mla_v3:/root/mla_v3" `
  -v mla-config:/mla_config `
  -p 8002:8002 `
  -p 9641:9641 `
  -p 4242:4242 `
  -p 5002:5002 `
  chenglinhku/mlav3:latest webui

# Then open browser: http://localhost:4242
# View logs: docker logs -f mla-webui
```

**4. Configure API Key**

Open browser: `http://localhost:9641`

<p align="center">
  <img src="assets/config_web_screen_shot.png" alt="Configuration Web Interface" width="800">
</p>

Edit `run_env_config/llm_config.yaml`, fill in your API key, and save.

**ğŸ‰ Done!** Start using MLA CLI.

ğŸ“– **[Complete Docker Guide](docs/DOCKER_GUIDE.md)**

---

### Option 2: Local Installation (Python Required)

**1. Install the package**

```bash
# Ensure Python version > 3.10
cd install_path
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
git clone https://github.com/ChenglinPoly/infiAgent.git
cd infiAgent
pip install -e .
```

**2. Install Playwright**

```bash
playwright install chromium
```

**3. Configure API Key**

```bash
mla-agent --config-set api_key "your-api-key"
```

**4. Start Tool Server**

```bash
mla-tool-server start
```

**5. Start CLI**

```bash
cd /your/workspace
mla-agent --cli
```

ğŸ“– **[Complete CLI Guide](docs/CLI_GUIDE.md)**

---

## ğŸ¯ How It Works

MLA's design philosophy is **"Provide short but high-value context for the next step."** To achieve this, the framework implements multiple innovations:

### 1. ğŸŒ² Serial Multi-Agent System

MLA deploys agents in a **tree-structured hierarchy** (e.g., Grandparent â†’ Parent â†’ Child). This ensures:

- âœ… **Single-purpose agents**: Each agent has a focused role
- âœ… **Minimal tool sets**: Agents only access necessary tools
- âœ… **Task alignment**: Serial execution prevents parallel conflicts
- âœ… **Clear delegation**: Parent agents orchestrate child agents

**Example Hierarchy:**
```
alpha_agent (Level 3)
  â”œâ”€â”€ data_collection_agent (Level 2)
  â”‚   â””â”€â”€ web_search_agent (Level 1)
  â”œâ”€â”€ coder_agent (Level 2)
  â””â”€â”€ material_to_document_agent (Level 2)
```

### 2. ğŸ¯ Nested Attention Mechanism

Long documents (PDFs, novels, papers) are **never directly loaded into context**. Instead:

- âœ… Use `answer_from_pdf`, `answer_from_document` tools
- âœ… Query-driven content extraction
- âœ… Only relevant excerpts or summaries enter context
- âœ… **Application-layer attention allocation** through tools

**Traditional Approach:**
```
Load entire 50-page PDF â†’ Agent processes everything â†’ Token overflow
```

**MLA Approach:**
```
Agent asks: "What is the methodology?"
â†’ Tool extracts relevant sections (2 pages)
â†’ Returns concise answer â†’ Minimal token usage
```

### 3. ğŸ“ File-Centric Architecture

**"Files are everything."** All outputs and interactions are saved to the file system:

- âœ… Web scraping â†’ Saves as Markdown files
- âœ… PDF parsing â†’ Extracts to structured documents
- âœ… Sub-agent results â†’ Stored as files
- âœ… **No immediate returns** cluttering context

**Benefits:**
- Clear audit trail
- Reusable artifacts
- Context-free state representation

### 4. âš¡ Ten-Step Strategy (No Context Compression)

A key insight: **The current file system state represents the effect of all historical actions.**

- âœ… A separate **thinking module** updates file space state every 10 steps
- âœ… Agents only retain **the last 10 actions** (since last state update)
- âœ… **No need for context compression**
- âœ… Historical actions are reflected in file system, not conversation history

**Traditional LLM Agents:**
```
Step 1: Create file A
Step 2: Edit file B
...
Step 100: Context overflow â†’ Compression needed â†’ Information loss
```

**MLA Approach:**
```
Steps 1-10: Actions recorded
Step 10: Thinking module updates "Current State: Files A, B, C exist with..."
Steps 11-20: Only these + Current State kept
â†’ No compression, no information loss
```

### 5. ğŸ”§ Batch File Operations

Inspired by [Claude Code](https://www.anthropic.com/), MLA uses **list-based tool parameters** to save tokens:

- âœ… Read multiple files in one call
- âœ… Batch operations reduce cumulative overhead
- âœ… Significant token savings on repeated actions

**Example:**
```python
# Traditional: 3 separate calls
file_read(path="file1.txt")
file_read(path="file2.txt")
file_read(path="file3.txt")

# MLA: 1 batch call
file_read(paths=["file1.txt", "file2.txt", "file3.txt"])
```

### 6. ğŸ’¾ Long-Term Memory with Task ID

- âœ… **Task ID = Workspace absolute path** (not user-configurable)
- âœ… Same task ID allows **unlimited conversation sessions**
- âœ… Agents remember all historical tasks in the workspace
- âœ… Persistent memory across interruptions and restarts

**Usage:**
```bash
# First session
mla-agent --task_id ~/research --user_input "Collect papers on Transformers"
# â†’ Stores conversation in ~/mla_v3/conversations/{hash}_research_*

# Second session (days later)
mla-agent --task_id ~/research --user_input "Summarize the collected papers"
# â†’ Agent remembers previous session and accesses collected files
```

### 7. ğŸ“Š Call Graph-Based Shared Context

The `hierarchy_manager` maintains a **dynamic call relationship graph**:

- âœ… Tracks parent-child agent relationships
- âœ… Injects call graph into shared context
- âœ… Prevents agents from overstepping boundaries
- âœ… Maintains task alignment across multi-agent system

**Call Graph Example:**
```json
{
  "current_agent": "coder_agent",
  "parent": "alpha_agent",
  "siblings": ["data_collection_agent", "material_to_document_agent"],
  "allowed_tools": ["python_run", "file_write", "file_read"]
}
```

This ensures `coder_agent` won't accidentally call `web_search` (not in its scope) or interfere with sibling agents.

---


## ğŸ“¸ Interface Screenshots

### CLI Interface

MLA provides a rich interactive CLI with real-time task monitoring, HIL handling, and agent switching:

**System Selection:**
<p align="center">
  <img src="assets/cli_choose_system.png" alt="CLI System Selection" width="800">
</p>

**Tool Mode Configuration:**
<p align="center">
  <img src="assets/cli_choose_tool_mode.png" alt="CLI Tool Mode" width="800">
</p>

**Starting Tasks:**
<p align="center">
  <img src="assets/cli_start_task.png" alt="CLI Task Execution" width="800">
</p>

*Interactive CLI with prompt_toolkit and rich terminal UI - featuring multi-turn conversations, automatic HIL detection, and tool execution confirmation.*

### VS Code Plugin

Build powerful IDE extensions using MLA's JSONL mode:

<p align="center">
  <img src="assets/vscode_plugin.png" alt="VS Code Plugin Screenshot" width="800">
</p>

*VS Code extension powered by MLA - seamless integration with workspace context and real-time streaming output.*

---

## âš™ï¸ Configuration Guide

MLA uses YAML files for agent and tool configuration. Configuration files are located in:

```
config/
â”œâ”€â”€ agent_library/
â”‚   â””â”€â”€ Default/                    # Default agent system
â”‚       â”œâ”€â”€ general_prompts.yaml    # Shared prompts
â”‚       â”œâ”€â”€ level_-1_judge_agent.yaml  # Judge agent
â”‚       â”œâ”€â”€ level_0_tools.yaml      # Tool definitions
â”‚       â”œâ”€â”€ level_1_agents.yaml     # Low-level agents
â”‚       â”œâ”€â”€ level_2_agents.yaml     # Mid-level agents
â”‚       â””â”€â”€ level_3_agents.yaml     # Top-level agents
â””â”€â”€ run_env_config/
    â”œâ”€â”€ llm_config.yaml             # LLM settings
    â””â”€â”€ tool_config.yaml            # Tool server settings
```

### Key Configuration Files

#### 1. `llm_config.yaml` - LLM Configuration

```yaml
api_key: "your-api-key"
base_url: "https://openrouter.ai/api/v1"
models:
  - "openai/anthropic/claude-sonnet-4"
  - "openai/anthropic/claude-haiku-4.5"
temperature: 0.7
max_tokens: 8000
figure_models:
  - "google/gemini-2.0-flash-thinking-exp-01-21"
```

**Note**: Copy `llm_config.example.yaml` to `llm_config.yaml` to get started.

#### 2. Agent Hierarchy

MLA organizes agents into levels:

- **Level 3**: Top-level orchestrators (e.g., `alpha_agent`)
- **Level 2**: Functional specialists (e.g., `data_collection_agent`, `coder_agent`)
- **Level 1**: Basic executors (e.g., `web_search_agent`)
- **Level 0**: Tool definitions
- **Level -1**: Quality control (e.g., `judge_agent`)

#### 3. Creating Custom Agents

Edit YAML files to customize agent behavior:

```yaml
news_agent:
  type: llm_call_agent
  level: 1
  model_type: "advanced"
  available_tools:
    - data_collection_agent
    - coder_agent
    ...
  system_prompt: |
    You are a newspaper agent.
```

---

## ğŸ’» CLI Interface

### Interactive Mode

Start the CLI for a conversational experience:

```bash
mla-agent --cli
```

**Key Features:**

- ğŸ”„ **Multi-turn conversations** with persistent context
- ğŸ¤– **Agent switching** with `@agent_name` syntax
- ğŸ”” **Automatic HIL detection** with audio alerts
- âš ï¸ **Tool execution confirmation** in manual mode
- â¸ï¸ **Interrupt and resume** support (Ctrl+C to pause)
- ğŸ¨ **Rich terminal UI** powered by `prompt_toolkit` and `rich`

**Usage Examples:**

```bash
# Direct task input (uses default agent)
[alpha_agent] > Collect papers on Transformers

# Switch agent and execute task
[alpha_agent] > @data_collection_agent Search for recent NLP papers

# Switch default agent only
[alpha_agent] > @coder_agent
âœ… Switched to: coder_agent
[coder_agent] > 
```

**CLI Commands:**

| Command | Description |
|---------|-------------|
| `/help` | Show help and available commands |
| `/agents` | List all available agents |
| `/resume` | Resume interrupted tasks |
| `/quit` or `/exit` | Exit CLI mode |
| `Ctrl+C` | Interrupt current task (stays in CLI) |
| `Ctrl+D` | Exit CLI immediately |

**Human-in-Loop (HIL) Handling:**

When an agent requests human input, the CLI automatically detects it:

```
ğŸ””ğŸ””ğŸ”” Detected HIL task! Press Enter to handle... ğŸ””ğŸ””ğŸ””
================================================================================
ğŸ”” Human Interaction Task (HIL)
================================================================================
ğŸ“ Task ID: upload_file_20250124
ğŸ“‹ Instruction: Please upload the required dataset files...
================================================================================
ğŸ’¡ Enter your response (any text)
   Type /skip to skip this task
================================================================================

[alpha_agent] HIL Response > Files uploaded successfully
âœ… HIL task responded
```

**Tool Confirmation (Manual Mode):**

When `--auto-mode false` is set, each tool execution requires confirmation:

```
âš ï¸âš ï¸âš ï¸ Detected tool execution request! Press Enter to confirm... âš ï¸âš ï¸âš ï¸
================================================================================
âš ï¸  Tool Execution Confirmation Request
================================================================================
ğŸ”§ Tool Name: python_run
ğŸ“ Confirmation ID: confirm_12345
ğŸ“‹ Parameters:
     code: import numpy as np...
     timeout: 300
================================================================================
ğŸ’¡ Choose action:
   yes / y - Approve execution
   no / n  - Reject execution
================================================================================

[alpha_agent] Confirm [yes/no] > yes
âœ… Approved tool execution: python_run
```

**Screenshot:** *(User will provide)*

---

### Command-Line Mode

For scripting and automation:

```bash
mla-agent \
  --task_id /path/to/workspace \
  --user_input "Your task description" \
  --agent_name alpha_agent
```

**Common Parameters:**

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--task_id` | Workspace path (absolute) | Required |
| `--user_input` | Task description | Required |
| `--agent_name` | Agent to invoke | `alpha_agent` |
| `--agent_system` | Agent library name | `Default` |
| `--cli` | Interactive CLI mode | `false` |
| `--jsonl` | JSONL output mode | `false` |
| `--force-new` | Clear all state and start fresh | `false` |
| `--auto-mode` | Tool execution mode (`true`/`false`) | Auto-detect |

**Auto-Mode Examples:**

```bash
# Automatic tool execution (no confirmation needed)
mla-agent --task_id ~/project --user_input "Task" --auto-mode true

# Manual confirmation for each tool
mla-agent --task_id ~/project --user_input "Task" --auto-mode false
```

---

### Managing Tool Server

```bash
# Start server (background)
mla-tool-server start

# Check status
mla-tool-server status

# Stop server
mla-tool-server stop

# Restart server
mla-tool-server restart
```

---

## ğŸ”Œ SDK Integration

MLA provides two SDK options: **Python SDK** for direct integration and **JSONL mode** for IDE plugins.

---

### Python SDK

Import and use MLA components directly in your Python code:

```python
from pathlib import Path
from utils.config_loader import ConfigLoader
from core.hierarchy_manager import get_hierarchy_manager
from core.agent_executor import AgentExecutor

# Initialize components
task_id = str(Path.home() / "my_project")
agent_system = "Default"

config_loader = ConfigLoader(agent_system)
hierarchy_manager = get_hierarchy_manager(task_id)

# Get agent configuration
agent_config = config_loader.get_tool_config("alpha_agent")

# Create and run agent
agent = AgentExecutor(
    agent_name="alpha_agent",
    agent_config=agent_config,
    config_loader=config_loader,
    hierarchy_manager=hierarchy_manager
)

# Execute task
result = agent.run(
    task_id=task_id,
    user_input="Write a survey paper on Transformers"
)

print(f"Status: {result['status']}")
print(f"Output: {result['output']}")
```

**Advanced: Custom Agent with Tool Permissions**

```python
# Set tool execution mode
agent.tool_executor.set_task_permission(task_id, auto_mode=True)

# Run with custom configuration
result = agent.run(task_id, user_input)

if result['status'] == 'success':
    print("Task completed successfully!")
else:
    print(f"Error: {result.get('error_information')}")
```

**Use Cases for Python SDK:**
- ğŸ”§ Building custom workflows
- ğŸ¤– Embedding agents in existing applications
- ğŸ“Š Batch processing multiple tasks
- ğŸ”¬ Research experiments with programmatic control

---

### JSONL Mode for IDE Plugins

MLA provides a JSONL streaming mode for real-time integration with IDEs and editors:

```bash
mla-agent \
  --task_id $(pwd) \
  --user_input "Optimize code performance" \
  --jsonl 2>/dev/null
```

**Output Format:**

```jsonl
{"type":"start","call_id":"c-1760936557-474c43","project":"~/project","agent":"alpha_agent","task":"Optimize..."}
{"type":"token","text":"[alpha_agent] Analyzing code..."}
{"type":"progress","phase":"execution","pct":30}
{"type":"token","text":"Calling tool: code_analyzer"}
{"type":"result","ok":true,"summary":"Optimization complete"}
{"type":"end","status":"ok","duration_ms":5432}
```

**Event Types:**

| Event Type | Description | Key Fields |
|------------|-------------|------------|
| `start` | Task begins | `call_id`, `agent`, `task` |
| `token` | Streaming text output | `text` |
| `progress` | Progress update | `phase`, `pct` |
| `result` | Task result | `ok`, `summary` |
| `end` | Task completed | `status`, `duration_ms` |
| `error` | Error occurred | `message` |

---

### TypeScript/JavaScript Integration

```typescript
import { spawn } from 'child_process';

interface AgentEvent {
  type: 'start' | 'token' | 'progress' | 'result' | 'end' | 'error';
  [key: string]: any;
}

function runAgent(
  workspacePath: string, 
  userInput: string,
  onEvent: (event: AgentEvent) => void
): Promise<AgentEvent> {
  return new Promise((resolve, reject) => {
  const child = spawn('mla-agent', [
    '--task_id', workspacePath,
    '--user_input', userInput,
    '--jsonl'
  ]);
  
    let buffer = '';
    
  child.stdout.on('data', (data) => {
      buffer += data.toString();
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';
      
      lines.forEach(line => {
      if (!line.trim()) return;
      
        try {
          const event: AgentEvent = JSON.parse(line);
          onEvent(event);
          
          if (event.type === 'end') {
            resolve(event);
          } else if (event.type === 'error') {
            reject(new Error(event.message));
          }
        } catch (e) {
          console.error('Failed to parse event:', line);
    }
  });
});

    child.stderr.on('data', (data) => {
      // Log errors to stderr
      console.error(data.toString());
    });
    
    child.on('error', reject);
  });
}

// Usage
await runAgent('/path/to/workspace', 'Write unit tests', (event) => {
      switch (event.type) {
        case 'start':
      console.log(`Task started: ${event.task}`);
          break;
        case 'token':
      process.stdout.write(event.text);
      break;
    case 'progress':
      updateProgressBar(event.pct);
          break;
        case 'result':
      console.log(`\nResult: ${event.summary}`);
          break;
      }
    });
```

---

### VS Code Extension Example

Build your own Cursor/VS Code extension using MLA:

**Extension Features:**
- ğŸ¤– Agent commands in command palette
- ğŸ’¬ Inline chat with workspace context
- ğŸ“ Automatic code generation and refactoring
- ğŸ” Literature search within editor
- ğŸ”” HIL task handling with UI prompts

**Basic Extension Structure:**

```typescript
// extension.ts
import * as vscode from 'vscode';
import { runAgent } from './mla-client';

export function activate(context: vscode.ExtensionContext) {
  let disposable = vscode.commands.registerCommand(
    'mla.executeTask', 
    async () => {
      const workspace = vscode.workspace.workspaceFolders?.[0].uri.fsPath;
      const input = await vscode.window.showInputBox({
        prompt: 'Enter task description'
      });
      
      if (!workspace || !input) return;
      
      // Show progress
      await vscode.window.withProgress({
        location: vscode.ProgressLocation.Notification,
        title: 'MLA Agent',
        cancellable: true
      }, async (progress, token) => {
        
        await runAgent(workspace, input, (event) => {
          if (event.type === 'token') {
            vscode.window.showInformationMessage(event.text);
          } else if (event.type === 'progress') {
            progress.report({ increment: event.pct });
          }
        });
      });
    }
  );
  
  context.subscriptions.push(disposable);
}
```

**Screenshot:** *(User will provide)*

---

## ğŸ“Š Example Outputs

### Academic Paper Output

MLA can generate complete research papers with the following structure:

```
upload/
â”œâ”€â”€ paper.tex               # Main LaTeX document
â”œâ”€â”€ references.bib          # Bibliography
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ results_comparison.png
â”‚   â””â”€â”€ ablation_study.png
â””â”€â”€ supplementary/
    â””â”€â”€ detailed_results.pdf
```

**Quality Metrics:**
- âœ… Passes peer review at EI/IEEE conferences
- âœ… Proper citation formatting
- âœ… High-quality figures (300 DPI)
- âœ… Coherent structure and flow

### Other Capabilities

**1. Scientific Computing**
- ECM protein composition simulation
- Logistics company shift scheduling
- Student assignment grading with feedback

**2. General Tasks**
- Web scraping and data extraction
- Code generation and debugging
- Document conversion and processing

---


## ğŸ“– Documentation

- [Tool Server API Documentation](tool_server_lite/README.md) - 18 available tools
- [Human-in-the-Loop API](tool_server_lite/HIL_API.md) - User interaction integration
- [Configuration Examples](config/agent_library/Default/) - Agent YAML templates

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

---

## ğŸ“„ License

 see [LICENSE](LICENSE) for details.

---

## ğŸ“„ Citation

If you use InfiAgent in your research, please cite our paper:

```bibtex
@article{yu2026infiagent,
  title={InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents},
  author={Yu, Chenglin and Wang, Yuchen and Wang, Songmiao and Yang, Hongxia and Li, Ming},
  journal={arXiv preprint arXiv:2601.03204},
  year={2026}
}
```

---

## ğŸ™ Acknowledgments

- Built with [LiteLLM](https://github.com/BerriAI/litellm) for unified LLM access
- Uses [Crawl4AI](https://github.com/unclecode/crawl4ai) for web scraping

---

## ğŸ“¬ Contact

**Author**: @yuchenglin

**Thanks to Contributors**ï¼š @wangyuchen @wangsongmiao @yuyang @lijinjia

**Email**: yuchenglin96@qq.com/cl0415@connect.hku.hk/chenglin.yu@poly.edu.h 

**GitHub**: [MLA V3 Repository](https://github.com/ChenglinPoly/infiAgent)

---
