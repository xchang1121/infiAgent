#!/usr/bin/env python3
from utils.windows_compat import safe_print
# -*- coding: utf-8 -*-
"""
Agentæ‰§è¡Œå™¨ - ä½¿ç”¨XMLç»“æ„åŒ–ä¸Šä¸‹æ–‡çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
"""

# Windowså…¼å®¹æ€§ï¼šè®¾ç½®UTF-8ç¼–ç 
try:
    from utils.windows_compat import setup_console_encoding
    setup_console_encoding()
except ImportError:
    pass

import json
from typing import Dict, List
from services.llm_client import SimpleLLMClient, ChatMessage
from core.context_builder import ContextBuilder
from core.tool_executor import ToolExecutor
from utils.event_emitter import get_event_emitter


class AgentExecutor:
    """Agentæ‰§è¡Œå™¨ - æ­£ç¡®çš„XMLä¸Šä¸‹æ–‡æ¶æ„"""
    
    def __init__(
        self,
        agent_name: str,
        agent_config: Dict,
        config_loader,
        hierarchy_manager
    ):
        """åˆå§‹åŒ–Agentæ‰§è¡Œå™¨"""
        self.agent_name = agent_name
        self.agent_config = agent_config
        self.config_loader = config_loader
        self.hierarchy_manager = hierarchy_manager
        
        # ä»é…ç½®ä¸­æå–ä¿¡æ¯
        self.available_tools = agent_config.get("available_tools", [])
        # self.max_turns = agent_config.get("max_turns", 100)
        self.max_turns = 10000000
        # æ¨¡å‹é€‰æ‹©é€»è¾‘
        requested_model = agent_config.get("model_type", "claude-3-7-sonnet-20250219")
        self.model_type = requested_model
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = SimpleLLMClient()
        self.llm_client.set_tools_config(config_loader.all_tools)
        
        # éªŒè¯å¹¶è°ƒæ•´æ¨¡å‹
        available_models = self.llm_client.models
        if self.model_type not in available_models:
            fallback_model = available_models[0]
            safe_print(f"âš ï¸è¯·æ±‚çš„æ¨¡å‹ '{self.model_type}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­")
            safe_print(f"âœ…ä½¿ç”¨å›é€€æ¨¡å‹: {fallback_model}")
            self.model_type = fallback_model
        else:
            safe_print(f"âœ…ä½¿ç”¨è¯·æ±‚çš„æ¨¡å‹: {self.model_type}")
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„é€ å™¨ï¼ˆè´Ÿè´£å®Œæ•´ä¸Šä¸‹æ–‡æ„å»ºï¼‰
        self.context_builder = ContextBuilder(
            hierarchy_manager,
            agent_config=agent_config,
            config_loader=config_loader,
            llm_client=self.llm_client,
            max_context_window=self.llm_client.max_context_window
        )
        
        # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨
        self.tool_executor = ToolExecutor(config_loader, hierarchy_manager)
        
        # åˆå§‹åŒ–å¯¹è¯å­˜å‚¨
        from utils.conversation_storage import ConversationStorage
        self.conversation_storage = ConversationStorage()
        
        # AgentçŠ¶æ€
        self.agent_id = None
        self.action_history = []  # æ¸²æŸ“ç”¨ï¼ˆä¼šå‹ç¼©ï¼‰
        self.action_history_fact = []  # å®Œæ•´è½¨è¿¹ï¼ˆä¸å‹ç¼©ï¼‰
        self.pending_tools = []  # å¾…æ‰§è¡Œçš„å·¥å…·ï¼ˆç”¨äºæ¢å¤ï¼‰
        self.latest_thinking = ""
        self.first_thinking_done = False
        self.thinking_interval = 10  # æ¯10è½®å·¥å…·è°ƒç”¨è§¦å‘ä¸€æ¬¡thinking
        self.tool_call_counter = 0
    
    def run(self, task_id: str, user_input: str) -> Dict:
        """æ‰§è¡ŒAgentä»»åŠ¡"""
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ¤– å¯åŠ¨Agent: {self.agent_name}")
        safe_print(f"ğŸ“ ä»»åŠ¡: {user_input[:100]}...")
        safe_print(f"{'='*80}\n")
        
        # å­˜å‚¨ task_input ä¾›å‹ç¼©å™¨ä½¿ç”¨
        self.current_task_input = user_input
        
        # Agentå…¥æ ˆ
        self.agent_id = self.hierarchy_manager.push_agent(self.agent_name, user_input)
        
        # å°è¯•åŠ è½½å·²æœ‰çš„å¯¹è¯å†å²
        loaded_data = self.conversation_storage.load_actions(task_id, self.agent_id)
        start_turn = 0
        if loaded_data:
            self.action_history = loaded_data.get("action_history", [])
            self.action_history_fact = loaded_data.get("action_history_fact", [])
            self.pending_tools = loaded_data.get("pending_tools", [])
            self.latest_thinking = loaded_data.get("latest_thinking", "")
            self.first_thinking_done = loaded_data.get("first_thinking_done", False)
            self.tool_call_counter = loaded_data.get("tool_call_counter", 0)
            start_turn = loaded_data.get("current_turn", 0) + 1
            safe_print(f"ğŸ“‚ å·²åŠ è½½å¯¹è¯å†å²ï¼Œä»ç¬¬ {start_turn + 1} è½®ç»§ç»­")
            safe_print(f"   æ¸²æŸ“å†å²: {len(self.action_history)}æ¡, å®Œæ•´è½¨è¿¹: {len(self.action_history_fact)}æ¡")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆï¼ˆæœ‰final_outputï¼‰
            for action in self.action_history_fact:
                if action.get("tool_name") == "final_output":
                    final_result = action.get("result", {})
                    safe_print(f"\nâœ… ä»»åŠ¡å·²å®Œæˆï¼Œç›´æ¥è¿”å›ä¹‹å‰çš„final_outputç»“æœ")
                    safe_print(f"   çŠ¶æ€: {final_result.get('status')}")
                    return final_result
            
            # æ¢å¤pendingå·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.pending_tools:
                safe_print(f"ğŸ”„ å‘ç°{len(self.pending_tools)}ä¸ªpendingå·¥å…·ï¼Œæ¢å¤æ‰§è¡Œ...")
                self._recover_pending_tools(task_id)
        
        # é¦–æ¬¡thinkingï¼ˆåˆå§‹è§„åˆ’ï¼‰
        if start_turn == 0 and not self.first_thinking_done:
            safe_print(f"[{self.agent_name}] å¼€å§‹è¡ŒåŠ¨å‰è¿›è¡Œåˆå§‹è§„åˆ’...")
            thinking_result = self._trigger_thinking(task_id, user_input, is_first=True)
            if thinking_result:
                self.latest_thinking = thinking_result
                self.first_thinking_done = True
                self.hierarchy_manager.update_thinking(self.agent_id, thinking_result)
                self._save_state(task_id, user_input, 0)
                safe_print(f"[{self.agent_name}] åˆå§‹è§„åˆ’å®Œæˆ")
                
                # å‘é€ thinking äº‹ä»¶ï¼ˆå®Œæ•´å†…å®¹ï¼‰
                emitter = get_event_emitter()
                if emitter.enabled:
                    emitter.token(f"[{self.agent_name}] åˆå§‹è§„åˆ’: {thinking_result}")
        
        # å¼ºåˆ¶å·¥å…·è°ƒç”¨è®¡æ•°å™¨
        max_tool_try = 0
        
        # æ‰§è¡Œå¾ªç¯
        for turn in range(start_turn, self.max_turns):
            safe_print(f"\n--- ç¬¬ {turn + 1}/{self.max_turns} è½®æ‰§è¡Œ ---")
            
            try:
                # æ¯è½®å¼€å§‹å‰ä¿å­˜çŠ¶æ€
                self._save_state(task_id, user_input, turn)
                
                # æ£€æŸ¥å¹¶å‹ç¼©å†å²åŠ¨ä½œï¼ˆå¦‚æœè¶…è¿‡é™åˆ¶ï¼‰
                self._compress_action_history_if_needed()
                
                # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«é€šç”¨prompts + åŠ¨æ€ä¸Šä¸‹æ–‡ï¼‰
                full_system_prompt = self.context_builder.build_context(
                    task_id,  # æ·»åŠ task_idå‚æ•°
                    self.agent_id,
                    self.agent_name,
                    user_input,
                    action_history=self.action_history  # ä¼ å…¥å½“å‰çš„åŠ¨ä½œå†å²
                )
                
                # è°ƒç”¨LLMï¼ˆhistoryæ°¸è¿œåªæœ‰ä¸€æ¡ï¼‰
                history = [ChatMessage(role="user", content="è¯·è¾“å‡ºä¸‹ä¸€ä¸ªåŠ¨ä½œ")]
                
                safe_print(f"ğŸ¤– è°ƒç”¨LLM: {self.model_type}")
                safe_print(f"   ğŸ“ System Prompté•¿åº¦: {len(full_system_prompt)} å­—ç¬¦")
                safe_print(f"   ğŸ”§ å¯ç”¨å·¥å…·: {len(self.available_tools)} ä¸ª")
                
                llm_response = self.llm_client.chat(
                    history=history,
                    model=self.model_type,
                    system_prompt=full_system_prompt,
                    tool_list=self.available_tools,
                    tool_choice="required"  # å¼ºåˆ¶å·¥å…·è°ƒç”¨
                )
                
                if llm_response.status != "success":
                    error_result = {
                        "status": "error",
                        "output": f"LLMè°ƒç”¨å¤±è´¥",
                        "error_information": llm_response.error_information
                    }
                    self.hierarchy_manager.pop_agent(self.agent_id, str(error_result))
                    return error_result
                
                safe_print(f"ğŸ“¥ LLMè¾“å‡º: {llm_response.output[:100]}...")
                safe_print(f"ğŸ”§ å·¥å…·è°ƒç”¨æ•°é‡: {len(llm_response.tool_calls)}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if not llm_response.tool_calls:
                    # å¼ºåˆ¶å·¥å…·è°ƒç”¨æœºåˆ¶
                    if max_tool_try < 5:
                        max_tool_try += 1
                        safe_print(f"âš ï¸ LLMæœªè°ƒç”¨å·¥å…·ï¼Œç¬¬{max_tool_try}/5æ¬¡æé†’")
                        # ä¸‹ä¸€è½®ä¼šåœ¨XMLä¸Šä¸‹æ–‡ä¸­çœ‹åˆ°ä¹‹å‰çš„å¤±è´¥è®°å½•
                        # å¯ä»¥é€‰æ‹©åœ¨action_historyä¸­æ·»åŠ ä¸€ä¸ªæ ‡è®°
                        self.action_history.append({
                            "tool_name": "_no_tool_call",
                            "arguments": {},
                            "result": {
                                "status": "error",
                                "output": f"ç¬¬{max_tool_try}æ¬¡ï¼šLLMæœªè°ƒç”¨å·¥å…·ï¼Œè¯·åœ¨ä¸‹ä¸€è½®ä¸­å¿…é¡»è°ƒç”¨å·¥å…·"
                            }
                        })
                        self._save_state(task_id, user_input, turn)
                        continue
                    else:
                        # 5æ¬¡åä»ä¸è°ƒç”¨ï¼Œè§¦å‘thinkingå¹¶æŠ¥é”™
                        safe_print("âŒ 5æ¬¡æé†’åä»æœªè°ƒç”¨å·¥å…·ï¼Œè§¦å‘thinkingåˆ†æ")
                        thinking_result = self._trigger_thinking(task_id, user_input, is_first=False)
                        
                        # å‘é€ thinking äº‹ä»¶ï¼ˆå®Œæ•´å†…å®¹ï¼‰
                        emitter = get_event_emitter()
                        if emitter.enabled:
                            emitter.warn(f"[{self.agent_name}] å¼ºåˆ¶thinking: {thinking_result if thinking_result else 'åˆ†æå¤±è´¥'}")
                        
                        error_result = {
                            "status": "error",
                            "output": thinking_result if thinking_result else "å¤šæ¬¡æœªè°ƒç”¨å·¥å…·",
                            "error_information": "Agentæ‹’ç»è°ƒç”¨å·¥å…·"
                        }
                        self.hierarchy_manager.pop_agent(self.agent_id, str(error_result))
                        return error_result
                
                # é‡ç½®è®¡æ•°å™¨ï¼ˆæˆåŠŸè°ƒç”¨äº†å·¥å…·ï¼‰
                max_tool_try = 0
                
                # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                for tool_call in llm_response.tool_calls:
                    safe_print(f"\nğŸ”§ æ‰§è¡Œå·¥å…·: {tool_call.name}")
                    safe_print(f"ğŸ“‹ å‚æ•°: {tool_call.arguments}")
                    
                    # å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶ï¼ˆJSONLæ¨¡å¼ï¼‰
                    emitter = get_event_emitter()
                    if emitter.enabled:
                        import json
                        params_str = json.dumps(tool_call.arguments, ensure_ascii=False, indent=2)
                        emitter.token(f"è°ƒç”¨å·¥å…·: {tool_call.name}\nå‚æ•°: {params_str}")
                    
                    # âœ… åœ¨ä¿å­˜ pending ä¹‹å‰ï¼Œä¸º level != 0 çš„å·¥å…·æ·»åŠ  uuid
                    arguments_with_uuid = self._add_uuid_if_needed(tool_call.name, tool_call.arguments)
                    
                    # âœ… å…ˆæ ‡è®°ä¸ºpendingï¼ˆä¿å­˜å¸¦ uuid çš„å‚æ•°ï¼‰
                    pending_tool = {
                        "id": tool_call.id,
                        "name": tool_call.name,
                        "arguments": arguments_with_uuid,
                        "status": "pending"
                    }
                    self.pending_tools.append(pending_tool)
                    self._save_state(task_id, user_input, turn)  # ä¿å­˜pendingçŠ¶æ€
                    
                    # æ‰§è¡Œå·¥å…·ï¼ˆä½¿ç”¨å¸¦ uuid çš„å‚æ•°ï¼‰
                    tool_result = self.tool_executor.execute(
                        tool_call.name,
                        arguments_with_uuid,
                        task_id
                    )
                    
                    # âœ… æ‰§è¡Œåä»pendingç§»é™¤
                    self.pending_tools = [t for t in self.pending_tools if t["id"] != tool_call.id]
                    
                    safe_print(f"âœ… ç»“æœ: {tool_result.get('status', 'unknown')}")
                    
                    # å‘é€å·¥å…·ç»“æœäº‹ä»¶ï¼ˆJSONLæ¨¡å¼ï¼‰
                    emitter = get_event_emitter()
                    if emitter.enabled:
                        status = tool_result.get('status', 'unknown')
                        output_preview = tool_result.get('output', '')[:100]
                        emitter.token(f"å·¥å…· {tool_call.name} å®Œæˆ: {status} - {output_preview}...")
                    
                    # è®°å½•åŠ¨ä½œåˆ°å†å²ï¼ˆä½¿ç”¨å¸¦ uuid çš„å‚æ•°ï¼‰
                    action_record = {
                        "tool_name": tool_call.name,
                        "arguments": arguments_with_uuid,
                        "result": tool_result
                    }
                    
                    # æ·»åŠ åˆ°å®Œæ•´è½¨è¿¹ï¼ˆæ°¸ä¸å‹ç¼©ï¼‰
                    self.action_history_fact.append(action_record)
                    
                    # æ·»åŠ åˆ°æ¸²æŸ“å†å²ï¼ˆä¼šè¢«å‹ç¼©ï¼‰
                    self.action_history.append(action_record)
                    
                    self.hierarchy_manager.add_action(self.agent_id, action_record)
                    
                    # å·¥å…·æ‰§è¡Œåä¿å­˜çŠ¶æ€
                    self._save_state(task_id, user_input, turn)
                    
                    # å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°
                    self.tool_call_counter += 1
                    
                    # å¦‚æœæ˜¯final_outputï¼Œè¿”å›ç»“æœ
                    if tool_call.name == "final_output":
                        safe_print(f"\n{'='*80}")
                        safe_print(f"âœ… Agentå®Œæˆ: {self.agent_name}")
                        safe_print(f"ğŸ“Š çŠ¶æ€: {tool_result.get('status', 'unknown')}")
                        safe_print(f"{'='*80}\n")
                        
                        self.hierarchy_manager.pop_agent(self.agent_id, tool_result.get("output", ""))
                        return tool_result
                
                # æ£€æŸ¥æ˜¯å¦è¯¥è§¦å‘thinkingï¼ˆæ¯Nè½®å·¥å…·è°ƒç”¨ï¼‰
                if self.tool_call_counter % self.thinking_interval == 0:
                    safe_print(f"[{self.agent_name}] ç¬¬{self.tool_call_counter}è½®å·¥å…·è°ƒç”¨ï¼Œè§¦å‘thinkingåˆ†æ")
                    thinking_result = self._trigger_thinking(task_id, user_input, is_first=False)
                    if thinking_result:
                        self.latest_thinking = thinking_result
                        self.hierarchy_manager.update_thinking(self.agent_id, thinking_result)
                        self._save_state(task_id, user_input, turn)
                        
                        # å‘é€ thinking äº‹ä»¶ï¼ˆå®Œæ•´å†…å®¹ï¼‰
                        emitter = get_event_emitter()
                        if emitter.enabled:
                            emitter.token(f"[{self.agent_name}] è¿›åº¦åˆ†æ: {thinking_result}")
                        safe_print(f"[{self.agent_name}] Thinkingåˆ†æå·²æ›´æ–°")
                        self.action_history=[]
            
            except Exception as e:
                safe_print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                safe_print(f"é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                
                error_result = {
                    "status": "error",
                    "output": f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™\n\nç›®å‰è¿›åº¦:\n{self.latest_thinking}" if self.latest_thinking else "æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™"
                }
                self.hierarchy_manager.pop_agent(self.agent_id, str(error_result))
                return error_result
        
        # è¶…è¿‡æœ€å¤§è½®æ¬¡
        safe_print(f"\nâš ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶: {self.max_turns}")
        timeout_result = {
            "status": "error",
            "output": "æ‰§è¡Œè¶…è¿‡æœ€å¤§è½®æ¬¡é™åˆ¶",
            "error_information": f"Max turns {self.max_turns} exceeded"
        }
        self.hierarchy_manager.pop_agent(self.agent_id, str(timeout_result))
        return timeout_result
    
    def _add_uuid_if_needed(self, tool_name: str, arguments: Dict) -> Dict:
        """
        ä¸º level != 0 çš„å·¥å…·æ·»åŠ  uuid åç¼€åˆ° task_input
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: åŸå§‹å‚æ•°
            
        Returns:
            å¤„ç†åçš„å‚æ•°ï¼ˆå¦‚æœéœ€è¦æ·»åŠ  uuidï¼Œè¿”å›æ–°å­—å…¸ï¼›å¦åˆ™è¿”å›åŸå­—å…¸ï¼‰
        """
        try:
            # è·å–å·¥å…·é…ç½®
            tool_config = self.config_loader.get_tool_config(tool_name)
            tool_level = tool_config.get("level", 0)
            tool_type = tool_config.get("type", "")
            
            # åªå¯¹ level != 0 çš„ llm_call_agent æ·»åŠ  uuid
            if tool_type == "llm_call_agent" and tool_level != 0 and "task_input" in arguments:
                import uuid
                # åˆ›å»ºæ–°å­—å…¸ï¼ˆé¿å…ä¿®æ”¹åŸå§‹å‚æ•°ï¼‰
                new_arguments = arguments.copy()
                original_input = arguments["task_input"]
                random_suffix = f" [call-{uuid.uuid4().hex[:8]}]"
                new_arguments["task_input"] = original_input + random_suffix
                safe_print(f"   ğŸ”– ä¸º level {tool_level} å·¥å…·æ·»åŠ  uuid åç¼€")
                return new_arguments
            
            # å…¶ä»–æƒ…å†µè¿”å›åŸå‚æ•°
            return arguments
        
        except Exception as e:
            safe_print(f"âš ï¸ æ·»åŠ  uuid æ—¶å‡ºé”™: {e}")
            return arguments
    
    def _trigger_thinking(self, task_id: str, task_input: str, is_first: bool = False) -> str:
        """
        è§¦å‘Thinking Agentè¿›è¡Œåˆ†æ
        
        Args:
            task_id: ä»»åŠ¡ID
            task_input: ä»»åŠ¡è¾“å…¥
            is_first: æ˜¯å¦æ˜¯é¦–æ¬¡thinking
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            from services.thinking_agent import ThinkingAgent
            
            thinking_agent = ThinkingAgent()
            
            # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
            full_system_prompt = self.context_builder.build_context(
                task_id,
                self.agent_id,
                self.agent_name,
                task_input,
                action_history=self.action_history
            )
            
            if is_first:
                # é¦–æ¬¡thinking - åˆå§‹è§„åˆ’
                return thinking_agent.analyze_first_thinking(
                    task_description=task_input,
                    agent_system_prompt=full_system_prompt,  # ä¼ å…¥å®Œæ•´çš„prompt
                    available_tools=self.available_tools,
                    tools_config=self.config_loader.all_tools  # ä¼ é€’å·¥å…·é…ç½®
                )
            else:
                return thinking_agent.analyze_first_thinking(
                    task_description=task_input,
                    agent_system_prompt=full_system_prompt,  # ä¼ å…¥å®Œæ•´çš„prompt
                    available_tools=self.available_tools,
                    tools_config=self.config_loader.all_tools  # ä¼ é€’å·¥å…·é…ç½®
                )
                # è¿›åº¦åˆ†æï¼ˆfull_system_promptå·²åŒ…å«<å†å²åŠ¨ä½œ>ï¼‰
                # return thinking_agent.analyze_progress(
                #     task_description=task_input,
                #     agent_system_prompt=full_system_prompt,  # å·²åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡
                #     tool_call_counter=self.tool_call_counter
                # )
        except Exception as e:
            safe_print(f"âš ï¸ Thinkingè§¦å‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _compress_action_history_if_needed(self):
        """æ£€æŸ¥å¹¶å‹ç¼©å†å²åŠ¨ä½œï¼ˆå¦‚æœè¶…è¿‡ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼‰"""
        if not self.action_history:
            return
        
        try:
            from services.action_compressor import ActionCompressor
            
            # åˆå§‹åŒ–å‹ç¼©å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not hasattr(self, 'action_compressor'):
                self.action_compressor = ActionCompressor(self.llm_client)
            
            # ä½¿ç”¨æ–°çš„å‹ç¼©ç­–ç•¥ï¼ˆä¼ å…¥ thinking å’Œ task_inputï¼‰
            compressed = self.action_compressor.compress_if_needed(
                self.action_history,
                self.llm_client.max_context_window,
                thinking=self.latest_thinking,
                task_input=getattr(self, 'current_task_input', '')
            )
            
            # å¦‚æœå‘ç”Ÿäº†å‹ç¼©ï¼Œæ›¿æ¢
            if len(compressed) < len(self.action_history):
                safe_print(f"âœ… å†å²åŠ¨ä½œå·²å‹ç¼©: {len(self.action_history)}æ¡ â†’ {len(compressed)}æ¡")
                self.action_history = compressed
        
        except Exception as e:
            safe_print(f"âš ï¸ å‹ç¼©å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _recover_pending_tools(self, task_id: str):
        """æ¢å¤pendingçŠ¶æ€çš„å·¥å…·è°ƒç”¨"""
        for pending_tool in self.pending_tools[:]:  # å¤åˆ¶åˆ—è¡¨
            try:
                safe_print(f"   ğŸ”„ æ¢å¤æ‰§è¡Œ: {pending_tool['name']}")
                safe_print(f"   ğŸ“‹ å‚æ•°: {pending_tool['arguments']}")
                
                # å‘é€æ¢å¤äº‹ä»¶ï¼ˆJSONLæ¨¡å¼ï¼‰
                emitter = get_event_emitter()
                if emitter.enabled:
                    import json
                    params_str = json.dumps(pending_tool["arguments"], ensure_ascii=False, indent=2)
                    emitter.token(f"æ¢å¤å·¥å…·: {pending_tool['name']}\nå‚æ•°: {params_str}")
                
                # é‡æ–°æ‰§è¡Œå·¥å…·
                tool_result = self.tool_executor.execute(
                    pending_tool["name"],
                    pending_tool["arguments"],
                    task_id
                )
                
                # è®°å½•ç»“æœ
                action_record = {
                    "tool_name": pending_tool["name"],
                    "arguments": pending_tool["arguments"],
                    "result": tool_result
                }
                
                self.action_history_fact.append(action_record)
                self.action_history.append(action_record)
                
                # ä»pendingç§»é™¤
                self.pending_tools.remove(pending_tool)
                
                safe_print(f"   âœ… æ¢å¤å®Œæˆ: {pending_tool['name']}")
                
                # å¦‚æœæ˜¯final_outputï¼Œç›´æ¥è¿”å›
                if pending_tool["name"] == "final_output":
                    return tool_result
            
            except Exception as e:
                safe_print(f"   âŒ æ¢å¤å¤±è´¥: {pending_tool['name']} - {e}")
        
        # æ¸…ç©ºpendingåˆ—è¡¨
        self.pending_tools = []
    
    def _save_state(self, task_id: str, user_input: str, current_turn: int):
        """
        ä¿å­˜å½“å‰çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            user_input: ç”¨æˆ·è¾“å…¥
            current_turn: å½“å‰è½®æ¬¡
        """
        # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«XMLä¸Šä¸‹æ–‡ï¼‰
        full_system_prompt = self.context_builder.build_context(
            task_id,  # æ·»åŠ task_idå‚æ•°
            self.agent_id,
            self.agent_name,
            user_input,
            action_history=self.action_history  # ä¼ å…¥åŠ¨ä½œå†å²
        )
        
        # ä¿å­˜çŠ¶æ€ï¼ˆæ–°æ ¼å¼ï¼‰
        self.conversation_storage.save_actions(
            task_id=task_id,
            agent_id=self.agent_id,
            agent_name=self.agent_name,
            task_input=user_input,
            action_history=self.action_history,  # æ¸²æŸ“ç”¨ï¼ˆä¼šå‹ç¼©ï¼‰
            action_history_fact=self.action_history_fact,  # å®Œæ•´è½¨è¿¹ï¼ˆä¸å‹ç¼©ï¼‰
            pending_tools=self.pending_tools,  # å¾…æ‰§è¡Œçš„å·¥å…·
            current_turn=current_turn,
            latest_thinking=self.latest_thinking,
            first_thinking_done=self.first_thinking_done,
            tool_call_counter=self.tool_call_counter,
            system_prompt=full_system_prompt
        )


if __name__ == "__main__":
    from utils.config_loader import ConfigLoader
    from core.hierarchy_manager import get_hierarchy_manager
    
    # æµ‹è¯•
    config_loader = ConfigLoader("infiHelper")
    hierarchy_manager = get_hierarchy_manager("test_task")
    
    hierarchy_manager.start_new_instruction("æµ‹è¯•ä»»åŠ¡")
    
    # è·å–writing_agenté…ç½®
    agent_config = config_loader.get_tool_config("alpha_agent")
    
    safe_print(f"âœ… Agenté…ç½®: {agent_config.get('name')}")
    safe_print(f"   Tools: {len(agent_config.get('available_tools', []))}")
