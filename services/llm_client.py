#!/usr/bin/env python3
from utils.windows_compat import safe_print
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - ä½¿ç”¨LiteLLMç»Ÿä¸€æ¥å£
"""

import os
import yaml
import time
import json
import concurrent.futures
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from litellm import completion  # ç›´æ¥å¯¼å…¥completionå‡½æ•°
import litellm


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str
    content: str


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨"""
    id: str          
    name: str
    arguments: Dict[str, Any]


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    status: str  # "success" or "error"
    output: str
    tool_calls: List[ToolCall]
    model: str
    finish_reason: str
    usage: Optional[Dict] = None
    error_information: str = ""


class SimpleLLMClient:
    """ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - åŸºäºLiteLLM"""
    
    def __init__(self, llm_config_path: str = None, tools_config_path: str = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
            tools_config_path: å·¥å…·é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½LLMé…ç½®
        if llm_config_path is None:
            project_root = Path(__file__).parent.parent
            llm_config_path = project_root / "config" / "run_env_config" / "llm_config.yaml"
        
        if not os.path.exists(llm_config_path):
            raise FileNotFoundError(f"LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {llm_config_path}")
        
        with open(llm_config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è¯»å–é…ç½®
        self.base_url = self.config.get("base_url", "")
        self.api_key = self.config.get("api_key", "")
        self.temperature = self.config.get("temperature", 0)
        self.max_tokens = self.config.get("max_tokens", 0)
        self.max_context_window = self.config.get("max_context_window", 100000)  # ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
        
        # è¯»å–è¶…æ—¶é…ç½®ï¼ˆé»˜è®¤å€¼ï¼š600s, 20s, 20sï¼‰
        self.timeout = self.config.get("timeout", 600)  # LiteLLM åŸç”Ÿï¼šæ€»è¶…æ—¶
        self.stream_timeout = self.config.get("stream_timeout", 20)  # LiteLLM åŸç”Ÿï¼šæµå¼è¶…æ—¶
        self.first_chunk_timeout = self.config.get("first_chunk_timeout", 20)  # åº”ç”¨å±‚å¼ºåˆ¶ï¼šé¦–åŒ…è¶…æ—¶
        
        # è§£ææ¨¡å‹é…ç½®ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        self.models = []  # æ¨¡å‹åç§°åˆ—è¡¨
        self.figure_models = []
        self.compressor_models = []
        self.model_configs = {}  # æ¨¡å‹åç§° -> é…ç½®å­—å…¸
        
        self._parse_models_config(self.config.get("models", []), self.models)
        self._parse_models_config(self.config.get("figure_models", []), self.figure_models)
        self._parse_models_config(self.config.get("compressor_models", []), self.compressor_models)

        
        if not self.api_key:
            raise ValueError("æœªé…ç½®APIå¯†é’¥")
        
        if not self.models:
            raise ValueError("æœªé…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        
        # åŠ è½½å·¥å…·é…ç½®
        self.tools_config = {}
        if tools_config_path and os.path.exists(tools_config_path):
            with open(tools_config_path, 'r', encoding='utf-8') as f:
                self.tools_config = yaml.safe_load(f)
        
        # é…ç½®LiteLLM
        litellm.set_verbose = False  # å…³é—­è¯¦ç»†æ—¥å¿—
        litellm.drop_params = True  # è‡ªåŠ¨ä¸¢å¼ƒä¸æ”¯æŒçš„å‚æ•°ï¼ˆå¦‚Anthropicä¸æ”¯æŒparallel_tool_callsï¼‰
        
        safe_print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆLiteLLMï¼‰")
        safe_print(f"   Base URL: {self.base_url}")
        safe_print(f"   å¯ç”¨æ¨¡å‹: {len(self.models)} ä¸ª")
        safe_print(f"   Figureæ¨¡å‹: {len(self.figure_models)} ä¸ª")
        safe_print(f"   Compressoræ¨¡å‹: {len(self.compressor_models)} ä¸ª")
        safe_print(f"   é»˜è®¤Temperature: {self.temperature}")
        safe_print(f"   é»˜è®¤Max Tokens: {self.max_tokens}")
        safe_print(f"   è¶…æ—¶é…ç½®: timeout={self.timeout}s, stream_timeout={self.stream_timeout}s, first_chunk_timeout={self.first_chunk_timeout}s")
    
    def _parse_models_config(self, models_config: List, target_list: List):
        """
        è§£ææ¨¡å‹é…ç½®ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. å­—ç¬¦ä¸²æ ¼å¼ï¼šç›´æ¥æ˜¯æ¨¡å‹åç§°
        2. å¯¹è±¡æ ¼å¼ï¼šåŒ…å« name å’Œé¢å¤–å‚æ•°
        
        Args:
            models_config: åŸå§‹æ¨¡å‹é…ç½®åˆ—è¡¨
            target_list: ç›®æ ‡åˆ—è¡¨ï¼ˆself.models, self.figure_models ç­‰ï¼‰
        """
        for model_item in models_config:
            if isinstance(model_item, str):
                # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯æ¨¡å‹åç§°
                target_list.append(model_item)
                self.model_configs[model_item] = {}
            elif isinstance(model_item, dict):
                # å¯¹è±¡æ ¼å¼ï¼šåŒ…å«é¢å¤–å‚æ•°
                model_name = model_item.get("name")
                if not model_name:
                    safe_print(f"âš ï¸ æ¨¡å‹é…ç½®ç¼ºå°‘ 'name' å­—æ®µï¼Œè·³è¿‡: {model_item}")
                    continue
                
                target_list.append(model_name)
                # ä¿å­˜é™¤ name å¤–çš„æ‰€æœ‰å‚æ•°
                extra_params = {k: v for k, v in model_item.items() if k != "name"}
                self.model_configs[model_name] = extra_params
                
                if extra_params:
                    safe_print(f"   ğŸ“ æ¨¡å‹ {model_name} é…ç½®äº†é¢å¤–å‚æ•°: {list(extra_params.keys())}")
            else:
                safe_print(f"âš ï¸ ä¸æ”¯æŒçš„æ¨¡å‹é…ç½®æ ¼å¼ï¼Œè·³è¿‡: {model_item}")
    
    def chat(
        self,
        history: List[ChatMessage],
        model: str,
        system_prompt: str,
        tool_list: List[str],
        tool_choice: str = "required",
        temperature: float = None,
        max_tokens: int = None,
        max_retries: int = 3
    ) -> LLMResponse:
        """
        è°ƒç”¨LLMè¿›è¡Œå¯¹è¯ (å¢å¼ºç‰ˆï¼šæ”¯æŒæµå¼ç›‘æ§ã€è‡ªåŠ¨é‡è¯•ã€å‚æ•°ä¿®å¤)
        
        Args:
            history: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_list: å¯ç”¨å·¥å…·åˆ—è¡¨
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            temperature: æ¸©åº¦å‚æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼Œå³æ€»å…±æœ€å¤š4æ¬¡å°è¯•ï¼‰
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
        if temperature is None:
            temperature = self.temperature
        if max_tokens is None:
            max_tokens = self.max_tokens
        
        # é‡è¯•å¾ªç¯
        last_error = None
        fixed_system_prompt = system_prompt  # å¯èƒ½ä¼šè¢«ä¿®å¤çš„ system prompt
        type_fix_attempted = False  # æ˜¯å¦å·²å°è¯•ç±»å‹ä¿®å¤
        
        for retry_count in range(max_retries + 1):
            if retry_count > 0:
                safe_print(f"   ğŸ”„ LLMé‡è¯• {retry_count}/{max_retries}...")
                time.sleep(2 * retry_count)  # æŒ‡æ•°é€€é¿ï¼š2ç§’, 4ç§’, 6ç§’
                
                # æ ¹æ®ä¸Šæ¬¡é”™è¯¯ç”Ÿæˆæç¤ºï¼ˆå¸®åŠ© LLM é¿å…é‡å¤é”™è¯¯ï¼‰
                if last_error:
                    retry_hint = self._generate_retry_hint(last_error.error_information, retry_count)
                    if retry_hint:
                        fixed_system_prompt = system_prompt + "\n\n" + retry_hint
                        safe_print(f"   ğŸ“ æ·»åŠ é”™è¯¯æé†’: {retry_hint[:80]}...")
            
            # è°ƒç”¨å†…éƒ¨å®ç°
            response = self._chat_internal(
                history, model, fixed_system_prompt, tool_list, 
                tool_choice, temperature, max_tokens
            )
            
            # å¦‚æœæˆåŠŸï¼Œç›´æ¥è¿”å›
            if response.status == "success":
                if retry_count > 0 or type_fix_attempted:
                    safe_print(f"   âœ… é‡è¯•æˆåŠŸ (ç¬¬{retry_count + 1}æ¬¡å°è¯•)")
                return response
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·å‚æ•°ç±»å‹é”™è¯¯ï¼ˆä¼˜å…ˆå¤„ç†ï¼Œä¸æ¶ˆè€—é‡è¯•æ¬¡æ•°ï¼‰
            if not type_fix_attempted and ("did not match schema" in response.error_information or "expected array, but got string" in response.error_information):
                safe_print(f"   ğŸ”§ æ£€æµ‹åˆ°å·¥å…·å‚æ•°ç±»å‹é”™è¯¯ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
                
                # å°è¯•ä¿®å¤ system promptï¼ˆæ·»åŠ å‚æ•°ç±»å‹æç¤ºï¼‰
                fix_hint = self._generate_type_fix_hint(response.error_information)
                if fix_hint:
                    fixed_system_prompt = system_prompt + "\n\n" + fix_hint
                    safe_print(f"   ğŸ“ å·²æ·»åŠ å‚æ•°ç±»å‹æç¤ºï¼Œç«‹å³é‡è¯•...")
                    type_fix_attempted = True
                    last_error = response
                    
                    # ç«‹å³é‡è¯•ï¼Œä¸è®¡å…¥retry_count
                    response = self._chat_internal(
                        history, model, fixed_system_prompt, tool_list, 
                        tool_choice, temperature, max_tokens
                    )
                    
                    if response.status == "success":
                        safe_print(f"   âœ… å‚æ•°ç±»å‹ä¿®å¤æˆåŠŸï¼")
                        return response
                    else:
                        safe_print(f"   âš ï¸ ä¿®å¤åä»å¤±è´¥ï¼Œç»§ç»­å¸¸è§„é‡è¯•...")
                        last_error = response
                        continue
            
            # æ‰€æœ‰é”™è¯¯éƒ½é‡è¯•ï¼ˆåŒ…æ‹¬APIä½™é¢ä¸è¶³ã€å¯†é’¥é”™è¯¯ç­‰ï¼‰
            error_type = self._get_error_type(response.error_information)
            safe_print(f"   âš ï¸ {error_type} (ç¬¬{retry_count + 1}æ¬¡)")
            last_error = response
            
            if retry_count < max_retries:
                continue  # ç»§ç»­é‡è¯•
            else:
                # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„é”™è¯¯
                safe_print(f"   âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries + 1})")
                return response
                # return response
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        safe_print(f"   âŒ LLMè°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries + 1}æ¬¡ï¼‰")
        return last_error
    
    def _chat_internal(
        self,
        history: List[ChatMessage],
        model: str,
        system_prompt: str,
        tool_list: List[str],
        tool_choice: str,
        temperature: float,
        max_tokens: int
    ) -> LLMResponse:
        """
        LLMè°ƒç”¨çš„å†…éƒ¨å®ç°ï¼ˆä½¿ç”¨ LiteLLM åŸç”Ÿè¶…æ—¶æœºåˆ¶ï¼‰
        """
        try:
            # æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰
            tools_definition = self._build_tools_definition(tool_list)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend([{"role": msg.role, "content": msg.content} for msg in history])
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "api_key": self.api_key,
                "stream": True,  # å¯ç”¨æµå¼æ¨¡å¼
                # --- LiteLLM åŸç”Ÿè¶…æ—¶è®¾å®šï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰---
                "timeout": self.timeout,              # å»ºç«‹è¿æ¥åŠæ•´ä½“å“åº”çš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
                "stream_timeout": self.stream_timeout,  # ä¸¤ä¸ªæµå¼æ•°æ®å—ï¼ˆchunkï¼‰ä¹‹é—´çš„æœ€å¤§é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            }
            
            # åªåœ¨ base_url éç©ºæ—¶æ·»åŠ  api_base
            if self.base_url:
                kwargs["api_base"] = self.base_url
            
            # åªåœ¨max_tokens > 0æ—¶æ·»åŠ 
            if max_tokens > 0:
                kwargs["max_tokens"] = max_tokens
            
            # æ·»åŠ å·¥å…·å®šä¹‰
            if tools_definition:
                kwargs["tools"] = tools_definition
                if tool_choice == "required":
                    kwargs["tool_choice"] = "required"
                kwargs["parallel_tool_calls"] = False
            elif tool_choice == "none":
                kwargs["tool_choice"] = "none"
            
            # æ·»åŠ æ¨¡å‹ç‰¹å®šçš„é¢å¤–å‚æ•°
            model_extra_params = self.model_configs.get(model, {})
            if model_extra_params:
                if "provider" in model_extra_params:
                    if "extra_body" not in kwargs:
                        kwargs["extra_body"] = {}
                    kwargs["extra_body"]["provider"] = model_extra_params["provider"]
                
                if "extra_headers" in model_extra_params:
                    kwargs["extra_headers"] = model_extra_params["extra_headers"]
                
                if "extra_body" in model_extra_params:
                    if "extra_body" not in kwargs:
                        kwargs["extra_body"] = {}
                    kwargs["extra_body"].update(model_extra_params["extra_body"])
            
            # å‘èµ·æµå¼è¯·æ±‚ï¼ˆLiteLLM ä¼šæ ¹æ® timeout å’Œ stream_timeout è‡ªåŠ¨ç®¡ç†è¶…æ—¶ï¼‰
            safe_print(f"   ğŸŒŠ æ­£åœ¨è°ƒç”¨LLM (timeout={kwargs['timeout']}s, stream_timeout={kwargs['stream_timeout']}s)...")
            safe_print(f"   ğŸ“¨ è¯·æ±‚æ¨¡å‹: {model}")
            safe_print(f"   ğŸ› ï¸ å·¥å…·æ•°é‡: {len(tools_definition)}")
            safe_print(f"   ğŸ“ æ¶ˆæ¯æ•°: {len(messages)}")
            request_start_time = time.time()
            
            # ç´¯ç§¯å˜é‡
            accumulated_content = ""
            accumulated_tool_calls = {}  # index -> {id, name, arguments}
            finish_reason = "unknown"
            response_model = model
            
            chunk_count = 0
            
            # --- å¼ºåˆ¶é¦–åŒ…è¶…æ—¶æ£€æµ‹ï¼ˆåŒ…å« completion è°ƒç”¨ä»¥é˜²æ­¢è¿æ¥æ± æ­»é”ï¼‰---
            try:
                # å®šä¹‰å®Œæ•´çš„åˆå§‹åŒ–å’Œé¦–åŒ…è·å–å‡½æ•°ï¼ˆé˜²æ­¢ httpx è¿æ¥æ± é”æ­»é”ï¼‰
                def get_response_and_first_chunk():
                    iterator = completion(**kwargs)
                    first = next(iterator)
                    return iterator, first
                
                # å¼ºåˆ¶é¦–åŒ…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒåŒ…å«è¿æ¥å»ºç«‹+é¦–åŒ…æ¥æ”¶ï¼Œé˜²æ­¢ httpx è¿æ¥æ± æ­»é”
                first_chunk_timeout = self.first_chunk_timeout  # ä»é…ç½®æ–‡ä»¶è¯»å–
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(get_response_and_first_chunk)
                    try:
                        # å¼ºåˆ¶ç­‰å¾…æ•´ä¸ªåˆå§‹åŒ–è¿‡ç¨‹ï¼ˆåŒ…æ‹¬ completion è°ƒç”¨ï¼‰
                        response_iterator, first_chunk = future.result(timeout=first_chunk_timeout)
                        
                        # å¤„ç†é¦–åŒ…
                        chunk_count += 1
                        latency = time.time() - request_start_time
                        safe_print(f"   âš¡ï¸ é¦–åŒ…å»¶è¿Ÿ: {latency:.2f}s")
                        
                        # å¤„ç†é¦–åŒ…é€»è¾‘
                        if hasattr(first_chunk, 'model'):
                            response_model = first_chunk.model
                        
                        # æ‰“å°é¦–åŒ…
                        try:
                            safe_print(f"\n[chunk #1] {first_chunk}", flush=True)
                        except Exception:
                            pass

                        if first_chunk.choices:
                            delta = first_chunk.choices[0].delta
                            if hasattr(delta, 'content') and delta.content:
                                accumulated_content += delta.content
                                try:
                                    safe_print(delta.content, end="", flush=True)
                                except Exception:
                                    pass
                            
                            if hasattr(delta, 'tool_calls') and delta.tool_calls:
                                for tc in delta.tool_calls:
                                    idx = tc.index
                                    if idx not in accumulated_tool_calls:
                                        accumulated_tool_calls[idx] = {"id": "", "name": "", "arguments": ""}
                                    if tc.id:
                                        accumulated_tool_calls[idx]["id"] = tc.id
                                    if tc.function and tc.function.name:
                                        accumulated_tool_calls[idx]["name"] += tc.function.name
                                    if tc.function and tc.function.arguments:
                                        accumulated_tool_calls[idx]["arguments"] += tc.function.arguments
                                    
                                    try:
                                        tc_args_preview = tc.function.arguments[:200] if tc.function and tc.function.arguments else ""
                                        safe_print(f"\n[tool_call #{idx}] {tc.function.name}: {tc_args_preview}", flush=True)
                                    except Exception:
                                        pass
                                        
                            if first_chunk.choices[0].finish_reason:
                                finish_reason = first_chunk.choices[0].finish_reason

                    except concurrent.futures.TimeoutError:
                        raise TimeoutError(f"è¿æ¥å»ºç«‹æˆ–é¦–åŒ…æ¥æ”¶è¶…æ—¶ï¼ˆè¶…è¿‡ {first_chunk_timeout}sï¼‰- å¯èƒ½åŸå› ï¼šhttpxè¿æ¥æ± æ­»é”ã€ç½‘ç»œæ–­å¼€ã€æœåŠ¡å™¨æ— å“åº”")
            
            except StopIteration:
                safe_print("   âš ï¸ å“åº”ä¸ºç©ºï¼ˆæ— æ•°æ®å—ï¼‰")
                return LLMResponse(
                    status="error",
                    output="",
                    tool_calls=[],
                    model=model,
                    finish_reason="empty",
                    error_information="Empty response - no chunks received"
                )
            
            # --- ç»§ç»­å¤„ç†å‰©ä½™ chunk ---
            # ç›´æ¥è¿­ä»£æµå¼å“åº”ï¼ˆLiteLLM ä¼šè‡ªåŠ¨å¤„ç†åç»­çš„ stream_timeoutï¼‰
            for chunk in response_iterator:
                chunk_count += 1
                # å…¨é‡æ‰“å° chunkï¼ˆæ–¹ä¾¿è§‚å¯Ÿæ–­è”å’Œå¢é‡å†…å®¹ï¼Œå¯èƒ½è¾ƒå™ªå£°ï¼‰
                try:
                    safe_print(f"\n[chunk #{chunk_count}] {chunk}", flush=True)
                except Exception:
                    pass
                
                # æå–æ¨¡å‹ä¿¡æ¯
                if hasattr(chunk, 'model'):
                    response_model = chunk.model
                
                if not chunk.choices:
                    continue
                    
                delta = chunk.choices[0].delta
                
                # A. ç´¯ç§¯æ–‡æœ¬å†…å®¹
                if hasattr(delta, 'content') and delta.content:
                    accumulated_content += delta.content
                    # ç›´æ¥æµå¼æ‰“å°æ¨¡å‹æ–‡æœ¬ç‰‡æ®µï¼Œä¾¿äºæ—  CLI æ—¶è§‚å¯Ÿè¿›åº¦
                    try:
                        safe_print(delta.content, end="", flush=True)
                    except Exception:
                        pass
                
                # B. ç´¯ç§¯å·¥å…·è°ƒç”¨
                if hasattr(delta, 'tool_calls') and delta.tool_calls:
                    for tc in delta.tool_calls:
                        idx = tc.index
                        if idx not in accumulated_tool_calls:
                            accumulated_tool_calls[idx] = {"id": "", "name": "", "arguments": ""}
                        
                        if tc.id:
                            accumulated_tool_calls[idx]["id"] = tc.id
                        if tc.function and tc.function.name:
                            accumulated_tool_calls[idx]["name"] += tc.function.name
                        if tc.function and tc.function.arguments:
                            accumulated_tool_calls[idx]["arguments"] += tc.function.arguments
                        
                        # å·¥å…·è°ƒç”¨æµå¼æ‰“å°ï¼šåç§°ä¸å‚æ•°å¢é‡ï¼Œä¾¿äºæ—  CLI æ—¶è·Ÿè¸ª
                        try:
                            tc_args_preview = tc.function.arguments[:200] if tc.function and tc.function.arguments else ""
                            safe_print(f"\n[tool_call #{idx}] {tc.function.name}: {tc_args_preview}", flush=True)
                        except Exception:
                            pass
                
                # C. è®°å½•ç»“æŸåŸå› 
                if chunk.choices[0].finish_reason:
                    finish_reason = chunk.choices[0].finish_reason
            
            safe_print(f"   âœ… æµå¼å“åº”å®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
            
            # æ„å»ºæœ€ç»ˆçš„ ToolCall å¯¹è±¡åˆ—è¡¨
            final_tool_calls = []
            for idx in sorted(accumulated_tool_calls.keys()):
                tc_data = accumulated_tool_calls[idx]
                
                try:
                    args_str = tc_data["arguments"]
                    if not args_str:
                        args = {}
                    else:
                        args = json.loads(args_str)
                except json.JSONDecodeError as e:
                    safe_print(f"\nâš ï¸ å·¥å…·å‚æ•°JSONè§£æå¤±è´¥: {str(e)}")
                    safe_print(f"   åŸå§‹å‚æ•°: {tc_data['arguments'][:200]}...")
                    
                    # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯
                    args = self._try_fix_json(tc_data["arguments"])
                    if args:
                        safe_print(f"   âœ… JSON è‡ªåŠ¨ä¿®å¤æˆåŠŸ")
                    else:
                        safe_print(f"   âŒ JSON ä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨ç©ºå‚æ•°")
                        args = {}
                
                final_tool_calls.append(ToolCall(
                    id=tc_data["id"] or f"call_{idx}",
                    name=tc_data["name"],
                    arguments=args
                ))
            
            return LLMResponse(
                status="success",
                output=accumulated_content,
                tool_calls=final_tool_calls,
                model=response_model,
                finish_reason=finish_reason
            )
        
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ŒåŒ…æ‹¬ LiteLLM æŠ›å‡ºçš„è¶…æ—¶å¼‚å¸¸
            error_msg = str(e)
            is_timeout = any(keyword in error_msg.lower() for keyword in ["timeout", "timed out", "time out"])
            
            if is_timeout:
                safe_print(f"â±ï¸  LLMè°ƒç”¨è¶…æ—¶ (åŸç”Ÿè¶…æ—¶æœºåˆ¶)")
                safe_print(f"   è¶…æ—¶è¯¦æƒ…: {error_msg}")
                safe_print(f"   ğŸ’¡ æç¤º: å¦‚æœé¢‘ç¹è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ï¼š")
                safe_print(f"      1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š")
                safe_print(f"      2. ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´ API å“åº”ç¼“æ…¢")
                safe_print(f"      3. API æœåŠ¡å•†é™æµæˆ–è¿‡è½½")
            else:
                safe_print(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {error_msg}")
            
            # è¿”å›åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯çš„å“åº”
            import traceback
            error_detail = traceback.format_exc()
            
            return LLMResponse(
                status="error",
                output="",
                tool_calls=[],
                model=model,
                finish_reason="timeout" if is_timeout else "error",
                error_information=f"{error_msg}\n\nDetails:\n{error_detail}"
            )
    
    def set_tools_config(self, tools_config: Dict):
        """
        è®¾ç½®å·¥å…·é…ç½®ï¼ˆä»ConfigLoaderä¼ å…¥ï¼‰
        
        Args:
            tools_config: å·¥å…·é…ç½®å­—å…¸
        """
        self.tools_config = tools_config
    
    def _try_fix_json(self, json_str: str) -> Dict:
        """
        å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯
        
        Args:
            json_str: å¯èƒ½æœ‰é—®é¢˜çš„ JSON å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        if not json_str or not json_str.strip():
            return {}
        
        try:
            # ç­–ç•¥ 1: å»é™¤å°¾éƒ¨å¤šä½™çš„é€—å·
            fixed = json_str.strip()
            if fixed.endswith(',}'):
                fixed = fixed[:-2] + '}'
            if fixed.endswith(',]'):
                fixed = fixed[:-2] + ']'
            
            # ç­–ç•¥ 2: è¡¥å…¨ç¼ºå¤±çš„ç»“æŸæ‹¬å·
            open_braces = fixed.count('{')
            close_braces = fixed.count('}')
            if open_braces > close_braces:
                fixed += '}' * (open_braces - close_braces)
            
            open_brackets = fixed.count('[')
            close_brackets = fixed.count(']')
            if open_brackets > close_brackets:
                fixed += ']' * (open_brackets - close_brackets)
            
            # ç­–ç•¥ 3: å°è¯•è§£æ
            result = json.loads(fixed)
            return result
        
        except Exception:
            # æ‰€æœ‰ä¿®å¤ç­–ç•¥éƒ½å¤±è´¥
            return None
    
    def _generate_type_fix_hint(self, error_info: str) -> str:
        """
        ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å‚æ•°ç±»å‹é”™è¯¯ï¼Œç”Ÿæˆä¿®å¤æç¤º
        
        Args:
            error_info: é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
            
        Returns:
            ä¿®å¤æç¤ºæ–‡æœ¬ï¼ˆæ·»åŠ åˆ° system promptï¼‰
        """
        try:
            import re
            
            # æå–å·¥å…·å
            tool_match = re.search(r"tool (\w+) did not match", error_info)
            if not tool_match:
                return ""
            tool_name = tool_match.group(1)
            
            # æå–æ‰€æœ‰å‚æ•°é”™è¯¯ï¼ˆæ”¯æŒå¤šä¸ªå‚æ•°åŒæ—¶å‡ºé”™ï¼‰
            param_errors = re.findall(r"`/([\w_]+)`:\s*expected\s+(\w+),\s*but\s+got\s+(\w+)", error_info)
            
            if not param_errors:
                return ""
            
            # åˆ†ç±»å¤„ç†
            null_params = []
            type_mismatches = []
            
            for param_name, expected_type, actual_type in param_errors:
                if actual_type == "null":
                    null_params.append(param_name)
                else:
                    type_mismatches.append((param_name, expected_type, actual_type))
            
            hints = []
            
            # å¤„ç† null å€¼é”™è¯¯
            if null_params:
                params_str = "ã€".join(null_params)
                hints.append(f"""
âš ï¸ å‚æ•° null å€¼é”™è¯¯ï¼š
å·¥å…· {tool_name} çš„å‚æ•° {params_str} è¢«è®¾ç½®ä¸º null

é‡è¦è§„åˆ™ï¼š
- å¯é€‰å‚æ•°å¦‚æœä¸éœ€è¦ï¼Œå¿…é¡»å®Œå…¨çœç•¥ï¼Œä¸è¦ä¼  nullï¼
- é”™è¯¯ç¤ºä¾‹: {{"path": "file.txt", "start_line": null}}  âŒ
- æ­£ç¡®ç¤ºä¾‹: {{"path": "file.txt"}}  âœ…
""")
            
            # å¤„ç†ç±»å‹ä¸åŒ¹é…é”™è¯¯
            for param_name, expected_type, actual_type in type_mismatches:
                safe_print(f"   ğŸ” æ£€æµ‹åˆ°: å·¥å…· {tool_name}, å‚æ•° {param_name}, éœ€è¦ {expected_type}, å¾—åˆ° {actual_type}")
                
                if expected_type == "array" and actual_type == "string":
                    hints.append(f"""
âš ï¸ å‚æ•°ç±»å‹é”™è¯¯ï¼š
å·¥å…· {tool_name} çš„å‚æ•° {param_name} å¿…é¡»æ˜¯æ•°ç»„ç±»å‹ï¼
- é”™è¯¯: {{"{param_name}": "value"}}  âŒ
- æ­£ç¡®: {{"{param_name}": ["value"]}}  âœ…
""")
                elif expected_type == "string" and actual_type == "array":
                    hints.append(f"""
âš ï¸ å‚æ•°ç±»å‹é”™è¯¯ï¼š
å·¥å…· {tool_name} çš„å‚æ•° {param_name} å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼
- é”™è¯¯: {{"{param_name}": ["value"]}}  âŒ
- æ­£ç¡®: {{"{param_name}": "value"}}  âœ…
""")
                else:
                    hints.append(f"""
âš ï¸ å‚æ•°ç±»å‹é”™è¯¯ï¼š
å·¥å…· {tool_name} çš„å‚æ•° {param_name} éœ€è¦ {expected_type}ï¼Œå®é™…å¾—åˆ° {actual_type}
""")
            
            return "\n".join(hints) if hints else ""
        
        except Exception as e:
            safe_print(f"   âš ï¸ ç”Ÿæˆä¿®å¤æç¤ºå¤±è´¥: {e}")
            return ""
    
    def _get_error_type(self, error_info: str) -> str:
        """
        ä»é”™è¯¯ä¿¡æ¯ä¸­æå–å‹å¥½çš„é”™è¯¯ç±»å‹æè¿°
        
        Args:
            error_info: é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
            
        Returns:
            å‹å¥½çš„é”™è¯¯ç±»å‹æè¿°
        """
        if "timeout" in error_info.lower() or "timed out" in error_info.lower():
            return "è¿æ¥è¶…æ—¶"
        elif "Internal Server Error" in error_info:
            return "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"
        elif "Failed to parse" in error_info and "JSON" in error_info:
            return "JSONæ ¼å¼é”™è¯¯"
        elif "expected integer, but got null" in error_info:
            return "å‚æ•°nullå€¼é”™è¯¯"
        elif "expected array, but got string" in error_info:
            return "å‚æ•°ç±»å‹é”™è¯¯(stringâ†’array)"
        elif "expected string, but got array" in error_info:
            return "å‚æ•°ç±»å‹é”™è¯¯(arrayâ†’string)"
        elif "did not match schema" in error_info:
            return "å‚æ•°æ ¡éªŒå¤±è´¥"
        elif "not in request.tools" in error_info:
            return "å·¥å…·ä¸å­˜åœ¨é”™è¯¯"
        elif "Invalid API key" in error_info or "api_key" in error_info.lower():
            return "APIå¯†é’¥é”™è¯¯"
        elif "rate limit" in error_info.lower():
            return "é€Ÿç‡é™åˆ¶"
        elif "insufficient" in error_info.lower() or "quota" in error_info.lower():
            return "ä½™é¢ä¸è¶³"
        else:
            return "æœªçŸ¥é”™è¯¯"
    
    def _generate_retry_hint(self, error_info: str, retry_count: int) -> str:
        """
        æ ¹æ®é”™è¯¯ä¿¡æ¯ç”Ÿæˆé‡è¯•æç¤ºï¼ˆæ·»åŠ åˆ° system promptï¼‰
        
        Args:
            error_info: é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            é‡è¯•æç¤ºæ–‡æœ¬
        """
        import re
        
        # 1. æœåŠ¡å™¨é”™è¯¯ - é™é»˜é‡è¯•ï¼ˆä¸éœ€è¦æç¤º LLMï¼‰
        if "Internal Server Error" in error_info:
            return ""
        
        # 2. null å€¼é”™è¯¯ - æœ€å¸¸è§
        if "but got null" in error_info:
            # å°è¯•æå–æ‰€æœ‰ null å‚æ•°
            null_params = re.findall(r"`/([\w_]+)`:\s*expected\s+\w+,\s*but\s+got\s+null", error_info)
            if null_params:
                params_str = "ã€".join(null_params)
                hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šå‚æ•° {params_str} è¢«è®¾ç½®ä¸º null

é‡è¦è§„åˆ™ï¼š
- å¯é€‰å‚æ•°å¦‚æœä¸éœ€è¦ï¼Œå¿…é¡»å®Œå…¨çœç•¥ï¼Œä¸è¦ä¼ é€’ null å€¼ï¼
- é”™è¯¯ç¤ºä¾‹: {{"path": "file.txt", "start_line": null}}  âŒ
- æ­£ç¡®ç¤ºä¾‹: {{"path": "file.txt"}}  âœ… (ç›´æ¥çœç•¥ start_line)

è¯·é‡æ–°ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œç¡®ä¿ä¸ä¼ é€’ null å€¼ã€‚
"""
                return hint
        
        # 3. JSON è§£æé”™è¯¯
        if "Failed to parse" in error_info and "JSON" in error_info:
            hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šå·¥å…·å‚æ•° JSON æ ¼å¼é”™è¯¯

JSON æ ¼å¼è¦æ±‚ï¼š
- æ‰€æœ‰é”®åå¿…é¡»ç”¨åŒå¼•å·ï¼š{{"key": "value"}}  âœ…  {{key: "value"}}  âŒ
- å­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨åŒå¼•å·ï¼š{{"path": "file.txt"}}  âœ…  {{"path": 'file.txt'}}  âŒ
- ä¸è¦æœ‰å°¾éƒ¨é€—å·ï¼š{{"a": 1, "b": 2}}  âœ…  {{"a": 1, "b": 2,}}  âŒ
- ç‰¹æ®Šå­—ç¬¦éœ€è¦è½¬ä¹‰ï¼š{{"path": "C:\\\\file.txt"}}  âœ…

è¯·é‡æ–°ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œç¡®ä¿ JSON æ ¼å¼æ­£ç¡®ã€‚
"""
            return hint
        
        # 4. å·¥å…·ä¸å­˜åœ¨é”™è¯¯
        if "not in request.tools" in error_info:
            # å°è¯•æå–å·¥å…·å
            tool_match = re.search(r"attempted to call tool ['\"](\w+)['\"]", error_info)
            wrong_tool = tool_match.group(1) if tool_match else "æŸä¸ªå·¥å…·"
            
            hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šå°è¯•è°ƒç”¨ä¸å­˜åœ¨çš„å·¥å…· '{wrong_tool}'

é‡è¦è§„åˆ™ï¼š
- åªèƒ½è°ƒç”¨æä¾›çš„å·¥å…·åˆ—è¡¨ä¸­çš„å·¥å…·
- ä¸è¦è‡ªå·±å‘æ˜æˆ–å‡è®¾å­˜åœ¨æŸä¸ªå·¥å…·
- ä»”ç»†æ£€æŸ¥å¯ç”¨å·¥å…·åˆ—è¡¨

è¯·é‡æ–°ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œåªä½¿ç”¨å·²æä¾›çš„å·¥å…·ã€‚
"""
            return hint
        
        # 5. ç±»å‹ä¸åŒ¹é…ï¼ˆarray vs stringï¼‰
        if "expected array, but got string" in error_info:
            tool_match = re.search(r"tool (\w+) did not match", error_info)
            param_match = re.search(r"`/([\w_]+)`:\s*expected array", error_info)
            
            tool_name = tool_match.group(1) if tool_match else "æŸå·¥å…·"
            param_name = param_match.group(1) if param_match else "æŸå‚æ•°"
            
            hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šå·¥å…· {tool_name} çš„å‚æ•° {param_name} ç±»å‹é”™è¯¯

ç±»å‹è¦æ±‚ï¼š
- å‚æ•° {param_name} å¿…é¡»æ˜¯æ•°ç»„ï¼ˆarrayï¼‰ç±»å‹
- é”™è¯¯ç¤ºä¾‹: {{"{param_name}": "value"}}  âŒ
- æ­£ç¡®ç¤ºä¾‹: {{"{param_name}": ["value"]}}  âœ…

è¯·é‡æ–°ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨æ•°ç»„æ ¼å¼ï¼ˆæ–¹æ‹¬å·åŒ…è£¹ï¼‰ã€‚
"""
            return hint
        
        # 6. API ä½™é¢/å¯†é’¥é”™è¯¯ - ä¹Ÿç»™æç¤ºï¼ˆè™½ç„¶é‡è¯•å¯èƒ½æ— æ•ˆï¼‰
        if "insufficient" in error_info.lower() or "quota" in error_info.lower():
            hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šAPI ä½™é¢ä¸è¶³æˆ–é…é¢å·²ç”¨å°½

è¿™å¯èƒ½æ˜¯ä¸´æ—¶é—®é¢˜ï¼Œæ­£åœ¨é‡è¯•...
å¦‚æœæŒç»­å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API è´¦æˆ·çŠ¶æ€ã€‚
"""
            return hint
        
        if "Invalid API key" in error_info or "api_key" in error_info.lower():
            hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼åŸå› ï¼šAPI å¯†é’¥é”™è¯¯æˆ–æ— æ•ˆ

è¿™å¯èƒ½æ˜¯ä¸´æ—¶é—®é¢˜ï¼Œæ­£åœ¨é‡è¯•...
å¦‚æœæŒç»­å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API å¯†é’¥é…ç½®ã€‚
"""
            return hint
        
        # 7. é€šç”¨æç¤º
        hint = f"""
âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•è­¦å‘Šï¼š
ä¸Šæ¬¡è°ƒç”¨å¤±è´¥ï¼é”™è¯¯ä¿¡æ¯ï¼š{error_info[:200]}

è¯·ä»”ç»†æ£€æŸ¥å·¥å…·è°ƒç”¨çš„æ ¼å¼ã€å‚æ•°ç±»å‹å’Œå€¼ï¼Œç¡®ä¿ç¬¦åˆå·¥å…·å®šä¹‰ã€‚
"""
        return hint
    
    def _build_tools_definition(self, tool_list: List[str]) -> List[Dict]:
        """æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰"""
        if not self.tools_config:
            return []
        
        tools = []
        for tool_name in tool_list:
            if tool_name in self.tools_config:
                tool_config = self.tools_config[tool_name]
                tools.append({
                    "type": "function",
                    "function": {
                        "name": tool_config.get("name", tool_name),
                        "description": tool_config.get("description", ""),
                        "parameters": tool_config.get("parameters", {})
                    }
                })
        
        return tools


if __name__ == "__main__":
    # æµ‹è¯•LLMå®¢æˆ·ç«¯
    try:
        client = SimpleLLMClient()
        safe_print(f"âœ… å¯ç”¨æ¨¡å‹: {client.models}")
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        history = [ChatMessage(role="user", content="è¯·è¾“å‡ºä¸‹ä¸€ä¸ªåŠ¨ä½œ")]
        response = client.chat(
            history=history,
            model=client.models[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            system_prompt="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
            tool_list=["file_read", "file_write"],
            tool_choice="required"
        )
        
        safe_print(f"âœ… å“åº”çŠ¶æ€: {response.status}")
        safe_print(f"âœ… å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
        if response.tool_calls:
            safe_print(f"âœ… ç¬¬ä¸€ä¸ªå·¥å…·: {response.tool_calls[0].name}")
    except Exception as e:
        safe_print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
