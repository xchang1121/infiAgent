#!/usr/bin/env python3
from utils.windows_compat import safe_print
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - ä½¿ç”¨LiteLLMç»Ÿä¸€æ¥å£
"""

import os
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from litellm import completion  # ç›´æ¥å¯¼å…¥completionå‡½æ•°
import litellm


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str
    content: str


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨"""
    id: str
    name: str
    arguments: Dict[str, Any]


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    status: str  # "success" or "error"
    output: str
    tool_calls: List[ToolCall]
    model: str
    finish_reason: str
    usage: Optional[Dict] = None
    error_information: str = ""


class SimpleLLMClient:
    """ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - åŸºäºLiteLLM"""
    
    def __init__(self, llm_config_path: str = None, tools_config_path: str = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
            tools_config_path: å·¥å…·é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½LLMé…ç½®
        if llm_config_path is None:
            project_root = Path(__file__).parent.parent
            llm_config_path = project_root / "config" / "run_env_config" / "llm_config.yaml"
        
        if not os.path.exists(llm_config_path):
            raise FileNotFoundError(f"LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {llm_config_path}")
        
        with open(llm_config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è¯»å–é…ç½®
        self.base_url = self.config.get("base_url", "")
        self.api_key = self.config.get("api_key", "")
        self.temperature = self.config.get("temperature", 0)
        self.max_tokens = self.config.get("max_tokens", 0)
        self.max_context_window = self.config.get("max_context_window", 100000)  # ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
        
        # è§£ææ¨¡å‹é…ç½®ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        self.models = []  # æ¨¡å‹åç§°åˆ—è¡¨
        self.figure_models = []
        self.compressor_models = []
        self.model_configs = {}  # æ¨¡å‹åç§° -> é…ç½®å­—å…¸
        
        self._parse_models_config(self.config.get("models", []), self.models)
        self._parse_models_config(self.config.get("figure_models", []), self.figure_models)
        self._parse_models_config(self.config.get("compressor_models", []), self.compressor_models)

        
        if not self.api_key:
            raise ValueError("æœªé…ç½®APIå¯†é’¥")
        
        if not self.models:
            raise ValueError("æœªé…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        
        # åŠ è½½å·¥å…·é…ç½®
        self.tools_config = {}
        if tools_config_path and os.path.exists(tools_config_path):
            with open(tools_config_path, 'r', encoding='utf-8') as f:
                self.tools_config = yaml.safe_load(f)
        
        # é…ç½®LiteLLM
        litellm.set_verbose = False  # å…³é—­è¯¦ç»†æ—¥å¿—
        litellm.drop_params = True  # è‡ªåŠ¨ä¸¢å¼ƒä¸æ”¯æŒçš„å‚æ•°ï¼ˆå¦‚Anthropicä¸æ”¯æŒparallel_tool_callsï¼‰
        
        safe_print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆLiteLLMï¼‰")
        safe_print(f"   Base URL: {self.base_url}")
        safe_print(f"   å¯ç”¨æ¨¡å‹: {len(self.models)} ä¸ª")
        safe_print(f"   Figureæ¨¡å‹: {len(self.figure_models)} ä¸ª")
        safe_print(f"   Compressoræ¨¡å‹: {len(self.compressor_models)} ä¸ª")
        safe_print(f"   é»˜è®¤Temperature: {self.temperature}")
        safe_print(f"   é»˜è®¤Max Tokens: {self.max_tokens}")
    
    def _parse_models_config(self, models_config: List, target_list: List):
        """
        è§£ææ¨¡å‹é…ç½®ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. å­—ç¬¦ä¸²æ ¼å¼ï¼šç›´æ¥æ˜¯æ¨¡å‹åç§°
        2. å¯¹è±¡æ ¼å¼ï¼šåŒ…å« name å’Œé¢å¤–å‚æ•°
        
        Args:
            models_config: åŸå§‹æ¨¡å‹é…ç½®åˆ—è¡¨
            target_list: ç›®æ ‡åˆ—è¡¨ï¼ˆself.models, self.figure_models ç­‰ï¼‰
        """
        for model_item in models_config:
            if isinstance(model_item, str):
                # ç®€å•æ ¼å¼ï¼šç›´æ¥æ˜¯æ¨¡å‹åç§°
                target_list.append(model_item)
                self.model_configs[model_item] = {}
            elif isinstance(model_item, dict):
                # å¯¹è±¡æ ¼å¼ï¼šåŒ…å«é¢å¤–å‚æ•°
                model_name = model_item.get("name")
                if not model_name:
                    safe_print(f"âš ï¸ æ¨¡å‹é…ç½®ç¼ºå°‘ 'name' å­—æ®µï¼Œè·³è¿‡: {model_item}")
                    continue
                
                target_list.append(model_name)
                # ä¿å­˜é™¤ name å¤–çš„æ‰€æœ‰å‚æ•°
                extra_params = {k: v for k, v in model_item.items() if k != "name"}
                self.model_configs[model_name] = extra_params
                
                if extra_params:
                    safe_print(f"   ğŸ“ æ¨¡å‹ {model_name} é…ç½®äº†é¢å¤–å‚æ•°: {list(extra_params.keys())}")
            else:
                safe_print(f"âš ï¸ ä¸æ”¯æŒçš„æ¨¡å‹é…ç½®æ ¼å¼ï¼Œè·³è¿‡: {model_item}")
    
    def chat(
        self,
        history: List[ChatMessage],
        model: str,
        system_prompt: str,
        tool_list: List[str],
        tool_choice: str = "required",
        temperature: float = None,
        max_tokens: int = None
    ) -> LLMResponse:
        """
        è°ƒç”¨LLMè¿›è¡Œå¯¹è¯
        
        Args:
            history: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_list: å¯ç”¨å·¥å…·åˆ—è¡¨
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            temperature: æ¸©åº¦å‚æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
        if temperature is None:
            temperature = self.temperature
        if max_tokens is None:
            max_tokens = self.max_tokens
        
        try:
            # æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰
            tools_definition = self._build_tools_definition(tool_list)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend([{"role": msg.role, "content": msg.content} for msg in history])
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "api_key": self.api_key,
            }
            
            # åªåœ¨ base_url éç©ºæ—¶æ·»åŠ  api_baseï¼ˆå¯¹äº Google/Anthropic ç­‰å®˜æ–¹ APIï¼Œç•™ç©ºè®© litellm è‡ªåŠ¨è·¯ç”±ï¼‰
            if self.base_url:
                kwargs["api_base"] = self.base_url
            
            # åªåœ¨max_tokens > 0æ—¶æ·»åŠ 
            if max_tokens > 0:
                kwargs["max_tokens"] = max_tokens
            
            # æ·»åŠ å·¥å…·å®šä¹‰
            if tools_definition:
                kwargs["tools"] = tools_definition
                if tool_choice == "required":
                    # litellm ä¼šè‡ªåŠ¨å°† tool_choice è½¬æ¢ä¸ºå„æ¨¡å‹çš„æ ¼å¼
                    # OpenAI: tool_choice="required"
                    # Gemini: tool_config={function_calling_config: {mode: "ANY"}}
                    kwargs["tool_choice"] = "required"
                # ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ¯æ¬¡åªè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼‰
                # æ³¨æ„ï¼šGemini ä¸æ”¯æŒ parallel_tool_callsï¼Œä½† litellm.drop_params=True ä¼šè‡ªåŠ¨ä¸¢å¼ƒ
                kwargs["parallel_tool_calls"] = False
            
            # æ·»åŠ æ¨¡å‹ç‰¹å®šçš„é¢å¤–å‚æ•°
            model_extra_params = self.model_configs.get(model, {})
            if model_extra_params:
                # å¤„ç† provider å‚æ•°ï¼ˆOpenRouter ç‰¹å®šï¼‰
                if "provider" in model_extra_params:
                    if "extra_body" not in kwargs:
                        kwargs["extra_body"] = {}
                    kwargs["extra_body"]["provider"] = model_extra_params["provider"]
                
                # å¤„ç† extra_headers
                if "extra_headers" in model_extra_params:
                    kwargs["extra_headers"] = model_extra_params["extra_headers"]
                
                # å¤„ç† extra_bodyï¼ˆåˆå¹¶åˆ°å·²æœ‰çš„ extra_bodyï¼‰
                if "extra_body" in model_extra_params:
                    if "extra_body" not in kwargs:
                        kwargs["extra_body"] = {}
                    kwargs["extra_body"].update(model_extra_params["extra_body"])
                
                safe_print(f"   âš™ï¸  åº”ç”¨æ¨¡å‹é¢å¤–å‚æ•°: {list(model_extra_params.keys())}")
            
            # ä½¿ç”¨LiteLLMè°ƒç”¨
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            safe_print(f"   ğŸ“ System Prompté•¿åº¦: {len(system_prompt)} å­—ç¬¦")
            safe_print(f"   ğŸ”§ å·¥å…·æ•°é‡: {len(tools_definition)}")
            safe_print(f"   ğŸ“¨ æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            response = completion(**kwargs)  # ä½¿ç”¨å¯¼å…¥çš„å‡½æ•°
            
            # è§£æå“åº”ï¼ˆå‚è€ƒåŸé¡¹ç›®çš„å®‰å…¨è§£ææ–¹å¼ï¼‰
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                message = choice.message
                
                output_text = message.content or ""
                tool_calls = []
                
                # å®‰å…¨è§£æå·¥å…·è°ƒç”¨
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tc in message.tool_calls:
                        import json
                        # å®‰å…¨è§£æå‚æ•°
                        try:
                            if isinstance(tc.function.arguments, str):
                                arguments = json.loads(tc.function.arguments)
                            else:
                                arguments = tc.function.arguments
                        except:
                            arguments = {}
                        
                        tool_calls.append(ToolCall(
                            id=tc.id,
                            name=tc.function.name,
                            arguments=arguments
                        ))
                
                # å®‰å…¨æå–usageä¿¡æ¯
                usage = None
                if hasattr(response, 'usage') and response.usage:
                    usage = {
                        "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                        "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                        "total_tokens": getattr(response.usage, 'total_tokens', 0)
                    }
            else:
                return LLMResponse(
                    status="error",
                    output="",
                    tool_calls=[],
                    model=model,
                    finish_reason="error",
                    error_information="å“åº”æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘choiceså­—æ®µ"
                )
            
            return LLMResponse(
                status="success",
                output=output_text,
                tool_calls=tool_calls,
                model=response.model,
                finish_reason=response.choices[0].finish_reason,
                usage=usage
            )
        
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return LLMResponse(
                status="error",
                output="",
                tool_calls=[],
                model=model,
                finish_reason="error",
                error_information=f"{str(e)}\n\nDetails:\n{error_detail}"
            )
    
    def set_tools_config(self, tools_config: Dict):
        """
        è®¾ç½®å·¥å…·é…ç½®ï¼ˆä»ConfigLoaderä¼ å…¥ï¼‰
        
        Args:
            tools_config: å·¥å…·é…ç½®å­—å…¸
        """
        self.tools_config = tools_config
    
    def _build_tools_definition(self, tool_list: List[str]) -> List[Dict]:
        """æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰"""
        if not self.tools_config:
            return []
        
        tools = []
        for tool_name in tool_list:
            if tool_name in self.tools_config:
                tool_config = self.tools_config[tool_name]
                tools.append({
                    "type": "function",
                    "function": {
                        "name": tool_config.get("name", tool_name),
                        "description": tool_config.get("description", ""),
                        "parameters": tool_config.get("parameters", {})
                    }
                })
        
        return tools


if __name__ == "__main__":
    # æµ‹è¯•LLMå®¢æˆ·ç«¯
    try:
        client = SimpleLLMClient()
        safe_print(f"âœ… å¯ç”¨æ¨¡å‹: {client.models}")
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        history = [ChatMessage(role="user", content="è¯·è¾“å‡ºä¸‹ä¸€ä¸ªåŠ¨ä½œ")]
        response = client.chat(
            history=history,
            model=client.models[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            system_prompt="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
            tool_list=["file_read", "file_write"],
            tool_choice="required"
        )
        
        safe_print(f"âœ… å“åº”çŠ¶æ€: {response.status}")
        safe_print(f"âœ… å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
        if response.tool_calls:
            safe_print(f"âœ… ç¬¬ä¸€ä¸ªå·¥å…·: {response.tool_calls[0].name}")
    except Exception as e:
        safe_print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
