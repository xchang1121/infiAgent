#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½»é‡çº§å¤šæ¨¡æ€LLMå®¢æˆ·ç«¯ - ä¸“ä¾›tool_serverä½¿ç”¨
æ”¯æŒï¼šæ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥
"""

import os
import yaml
import base64
from pathlib import Path
from typing import Optional
from litellm import completion
import litellm

# å°è¯•å¯¼å…¥ transcribeï¼Œå¦‚æœä¸æ”¯æŒåˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    from litellm import transcribe
    HAS_TRANSCRIBE = True
except ImportError:
    HAS_TRANSCRIBE = False
    # å¦‚æœæ²¡æœ‰transcribeï¼Œéœ€è¦ä½¿ç”¨openaiç›´æ¥è°ƒç”¨
    try:
        import openai
        HAS_OPENAI = True
    except ImportError:
        HAS_OPENAI = False


class LLMClientLite:
    """è½»é‡çº§å¤šæ¨¡æ€LLMå®¢æˆ·ç«¯ - ä¾›tool_serverå·¥å…·ä½¿ç”¨"""
    
    def __init__(self, llm_config_path: str = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤è¯»å–é¡¹ç›®é…ç½®
        """
        # åŠ è½½LLMé…ç½®
        if llm_config_path is None:
            # ä»tool_server_liteç›®å½•æ‰¾åˆ°config
            current_dir = Path(__file__).parent
            config_path = current_dir.parent / "config" / "run_env_config" / "llm_config.yaml"
            
            if not config_path.exists():
                raise FileNotFoundError(f"LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            
            llm_config_path = str(config_path)
        
        if not os.path.exists(llm_config_path):
            raise FileNotFoundError(f"LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {llm_config_path}")
        
        # ä¿å­˜é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåç»­å¯èƒ½çš„é‡è½½ï¼‰
        self.config_path = llm_config_path
        
        with open(llm_config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è¯»å–é…ç½®
        self.base_url = self.config.get("base_url", "")
        self.api_key = self.config.get("api_key", "")
        self.models = self.config.get("models", [])
        self.figure_models = self.config.get("figure_models", [])
        self.compressor_models = self.config.get("compressor_models", [])
        self.read_figure_models = self.config.get("read_figure_models", [])
        self.temperature = self.config.get("temperature", 0)
        self.max_tokens = self.config.get("max_tokens", 0)
        
        if not self.api_key:
            raise ValueError("æœªé…ç½®APIå¯†é’¥")
        
        if not self.models:
            raise ValueError("æœªé…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        
        # é…ç½®LiteLLM
        litellm.set_verbose = False
        litellm.drop_params = True
        
        print(f"âœ… LLMå®¢æˆ·ç«¯é…ç½®å·²åŠ è½½: {llm_config_path}")
    
    def reload_config(self):
        """
        é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶
        
        ç”¨äºåœ¨è¿è¡Œæ—¶æ›´æ–°é…ç½®è€Œæ— éœ€é‡å¯æœåŠ¡
        """
        print(f"ğŸ”„ é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
        
        if not os.path.exists(self.config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # æ›´æ–°é…ç½®
        self.base_url = self.config.get("base_url", "")
        self.api_key = self.config.get("api_key", "")
        self.models = self.config.get("models", [])
        self.figure_models = self.config.get("figure_models", [])
        self.compressor_models = self.config.get("compressor_models", [])
        self.read_figure_models = self.config.get("read_figure_models", [])
        self.temperature = self.config.get("temperature", 0)
        self.max_tokens = self.config.get("max_tokens", 0)
        
        if not self.api_key:
            raise ValueError("æœªé…ç½®APIå¯†é’¥")
        
        if not self.models:
            raise ValueError("æœªé…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        
        print(f"âœ… é…ç½®å·²é‡æ–°åŠ è½½")
    
    def vision_query(
        self,
        image_path: str,
        question: str = "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹",
        model: Optional[str] = None
    ) -> str:
        """
        è°ƒç”¨Visionæ¨¡å‹åˆ†æå›¾ç‰‡
        
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
            question: è¦é—®çš„é—®é¢˜
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            
        Returns:
            LLMçš„å“åº”æ–‡æœ¬
            
        Raises:
            FileNotFoundError: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨
            Exception: LLMè°ƒç”¨å¤±è´¥
        """
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
        img_path = Path(image_path)
        if not img_path.exists():
            raise FileNotFoundError(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        
        # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
        with open(img_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # åˆ¤æ–­å›¾ç‰‡æ ¼å¼
        suffix = img_path.suffix.lower()
        mime_type_map = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        mime_type = mime_type_map.get(suffix, 'image/jpeg')
        
        # æ„å»ºVisionæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": question
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{image_data}"
                    }
                }
            ]
        }]
        
        # é€‰æ‹©æ¨¡å‹
        if model is None:
            model = self.read_figure_models[0]
        
        # è°ƒç”¨LLM
        try:
            response = completion(
                model=model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.base_url
            )
            
            # æå–å“åº”
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                raise Exception("LLMå“åº”æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘choiceså­—æ®µ")
                
        except Exception as e:
            raise Exception(f"è°ƒç”¨LLM Vision APIå¤±è´¥: {str(e)}")

    def create_image(
        self,
        prompt: str,
        model: Optional[str] = None
    ) -> str:
        """
        è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›¾ç‰‡
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ figure_models ä¸­çš„ç¬¬ä¸€ä¸ª
            
        Returns:
            å›¾ç‰‡çš„ base64 æ•°æ® URLï¼ˆæ ¼å¼ï¼šdata:image/png;base64,...ï¼‰æˆ– HTTP URL
            
        Note:
            - OpenRouter: ä½¿ç”¨ chat.completions + modalities
            - å…¶ä»– API: ä½¿ç”¨ litellm.image_generation()
        """
        if model is None:
            if self.figure_models:
                # å…¼å®¹å­—ç¬¦ä¸²æˆ–å­—å…¸æ ¼å¼
                first_model = self.figure_models[0]
                model = first_model if isinstance(first_model, str) else first_model.get("name")
            else:
                model = "dall-e-3"
        
        try:
            print(f"[INFO] è°ƒç”¨å›¾ç‰‡ç”Ÿæˆ API: {model}")
            if self.base_url:
                print(f"[INFO] ä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹: {self.base_url}")
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯ OpenRouter
            is_openrouter = self.base_url and 'openrouter' in self.base_url.lower()
            
            if is_openrouter:
                # OpenRouter ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ chat.completions + modalities
                from openai import OpenAI
                
                print(f"[INFO] ä½¿ç”¨ OpenRouter å›¾ç‰‡ç”Ÿæˆæ–¹å¼")
                
                client = OpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                )
                
                # ä½¿ç”¨ chat.completions API
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    extra_body={"modalities": ["image", "text"]}
                )
                
                # æå–å›¾ç‰‡
                message = response.choices[0].message
                if hasattr(message, 'images') and message.images:
                    for image in message.images:
                        if isinstance(image, dict):
                            image_url = image.get('image_url', {}).get('url')
                            if image_url:
                                print(f"[INFO] æˆåŠŸç”Ÿæˆå›¾ç‰‡: {image_url[:50]}...")
                                return image_url
                    raise Exception("images åˆ—è¡¨ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾ç‰‡ URL")
                else:
                    raise Exception(f"å“åº”ä¸­æ²¡æœ‰ images å­—æ®µã€‚Message å±æ€§: {dir(message)}")
            
            else:
                # å…¶ä»– APIï¼šä½¿ç”¨æ ‡å‡†çš„ image_generation
                from litellm import image_generation
                
                print(f"[INFO] ä½¿ç”¨æ ‡å‡† image_generation() æ–¹å¼")
                
                # æ„å»ºå‚æ•°
                kwargs = {
                    "model": model,
                    "prompt": prompt,
                    "api_key": self.api_key,
                }
                
                # åªæœ‰åœ¨æœ‰è‡ªå®šä¹‰ base_url æ—¶æ‰æ·»åŠ  api_base å‚æ•°
                if self.base_url and self.base_url.strip():
                    kwargs["api_base"] = self.base_url
                
                # è°ƒç”¨ litellm.image_generation
                response = image_generation(**kwargs)
                
                # è§£æå“åº”
                if response.data and len(response.data) > 0:
                    first_image = response.data[0]
                    
                    # ä¼˜å…ˆè¿”å› URL
                    if hasattr(first_image, 'url') and first_image.url:
                        print(f"[INFO] æˆåŠŸç”Ÿæˆå›¾ç‰‡: {first_image.url[:100]}...")
                        return first_image.url
                    # å…¶æ¬¡è¿”å› base64
                    elif hasattr(first_image, 'b64_json') and first_image.b64_json:
                        data_url = f"data:image/png;base64,{first_image.b64_json}"
                        print(f"[INFO] æˆåŠŸç”Ÿæˆå›¾ç‰‡ï¼ˆbase64ï¼‰ï¼Œé•¿åº¦: {len(data_url)}")
                        return data_url
                    else:
                        raise Exception(f"å›¾ç‰‡å“åº”æ ¼å¼å¼‚å¸¸: {first_image}")
                else:
                    raise Exception("æ¨¡å‹æœªè¿”å›å›¾ç‰‡æ•°æ®")
                
        except Exception as e:
            raise Exception(f"ç”Ÿæˆå›¾ç‰‡å¤±è´¥: {str(e)}")
    
    def audio_query(
        self,
        audio_path: str,
        question: str = "è¯·æè¿°è¿™æ®µéŸ³é¢‘çš„å†…å®¹",
        model: Optional[str] = None
    ) -> str:
        """
        è°ƒç”¨Audioæ¨¡å‹åˆ†æéŸ³é¢‘
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
            question: è¦é—®çš„é—®é¢˜
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            
        Returns:
            LLMçš„å“åº”æ–‡æœ¬ï¼ˆåŒ…å«è½¬å½•å†…å®¹å’Œåˆ†æç»“æœï¼‰
            
        Raises:
            FileNotFoundError: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨
            Exception: LLMè°ƒç”¨å¤±è´¥
        
        æµç¨‹:
        1. ä½¿ç”¨ Whisper API å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬
        2. æ ¹æ®é—®é¢˜åˆ†æè½¬å½•å†…å®¹å¹¶è¿”å›ç»“æœ
        """
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
        audio_file = Path(audio_path)
        if not audio_file.exists():
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        # åˆ¤æ–­éŸ³é¢‘æ ¼å¼
        suffix = audio_file.suffix.lower()
        supported_formats = {
            '.mp3': 'audio/mpeg',
            '.mp4': 'audio/mp4',
            '.mpeg': 'audio/mpeg',
            '.mpga': 'audio/mpeg',
            '.m4a': 'audio/mp4',
            '.wav': 'audio/wav',
            '.webm': 'audio/webm'
        }
        
        if suffix not in supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: {suffix}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats.keys())}")
        
        # é€‰æ‹©æ¨¡å‹
        if model is None:
            model = self.models[0]
        
        try:
            # æ­¥éª¤1: è½¬å½•éŸ³é¢‘ä¸ºæ–‡æœ¬
            print(f"ğŸ“ æ­£åœ¨è½¬å½•éŸ³é¢‘: {audio_path}")
            
            transcript_text = ""
            
            if HAS_TRANSCRIBE:
                # ä½¿ç”¨ litellm çš„ transcribe åŠŸèƒ½
                transcript = litellm.transcribe(
                    model="whisper-1",
                    file=str(audio_file),
                    api_key=self.api_key,
                    api_base=self.base_url
                )
                
                # æå–è½¬å½•æ–‡æœ¬
                if isinstance(transcript, dict) and 'text' in transcript:
                    transcript_text = transcript['text']
                elif isinstance(transcript, str):
                    transcript_text = transcript
                else:
                    transcript_text = str(transcript)
            
            elif HAS_OPENAI:
                # ä½¿ç”¨ OpenAI ç›´æ¥è°ƒç”¨
                with open(audio_file, "rb") as f:
                    transcript = openai.Audio.transcribe(
                        "whisper-1",
                        f,
                        api_key=self.api_key,
                        api_base=self.base_url if self.base_url else None
                    )
                    transcript_text = transcript['text']
            
            else:
                raise Exception("æœªå®‰è£…å¿…è¦çš„åº“ï¼ˆlitellm æˆ– openaiï¼‰")
            
            print(f"âœ… è½¬å½•å®Œæˆï¼Œæ–‡æœ¬é•¿åº¦: {len(transcript_text)} å­—ç¬¦")
            
            # æ­¥éª¤2: å¯¹è½¬å½•å†…å®¹è¿›è¡Œåˆ†æ
            messages = [{
                "role": "user",
                "content": f"ä»¥ä¸‹æ˜¯éŸ³é¢‘è½¬å½•å†…å®¹ï¼š\n\n{transcript_text}\n\nè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
            }]
            
            response = completion(
                model=model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.base_url
            )
            
            # æå–å“åº”
            if response.choices and len(response.choices) > 0:
                analysis_result = response.choices[0].message.content
                
                # è¿”å›åŒ…å«è½¬å½•å’Œåˆ†æçš„å®Œæ•´ç»“æœ
                return f"ã€éŸ³é¢‘è½¬å½•ã€‘\n{transcript_text}\n\nã€åˆ†æç»“æœã€‘\n{analysis_result}"
            else:
                raise Exception("LLMå“åº”æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘choiceså­—æ®µ")
                
        except Exception as e:
            raise Exception(f"è°ƒç”¨éŸ³é¢‘åˆ†æAPIå¤±è´¥: {str(e)}")
    
    def text_query(
        self,
        text: str,
        question: str,
        model: Optional[str] = None
    ) -> str:
        """
        é€šç”¨æ–‡æœ¬åˆ†æï¼ˆé€‚ç”¨äºè®ºæ–‡ã€æ–‡æ¡£ç­‰é•¿æ–‡æœ¬ï¼‰
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
            question: é—®é¢˜æˆ–æŒ‡ä»¤
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            
        Returns:
            LLMçš„å“åº”æ–‡æœ¬
            
        Raises:
            Exception: LLMè°ƒç”¨å¤±è´¥
        """
        # æ„å»ºæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": f"ä»¥ä¸‹æ˜¯å†…å®¹ï¼š\n\n{text}\n\n{question}"
        }]
        
        # é€‰æ‹©æ¨¡å‹
        if model is None:
            model = self.models[0]
        
        # è°ƒç”¨LLM
        try:
            response = completion(
                model=model,
                messages=messages,
                temperature=self.temperature,
                api_key=self.api_key,
                api_base=self.base_url
            )
            
            # æå–å“åº”
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                raise Exception("LLMå“åº”æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘choiceså­—æ®µ")
                
        except Exception as e:
            raise Exception(f"è°ƒç”¨LLMæ–‡æœ¬åˆ†æAPIå¤±è´¥: {str(e)}")


# å…¨å±€å•ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶çƒ­é‡è½½ï¼‰
_client_instance: Optional[LLMClientLite] = None
_config_file_path: Optional[str] = None
_config_file_mtime: Optional[float] = None


def get_llm_client(force_reload: bool = False) -> LLMClientLite:
    """
    è·å–LLMå®¢æˆ·ç«¯å•ä¾‹ï¼ˆæ”¯æŒé…ç½®æ–‡ä»¶çƒ­é‡è½½ï¼‰
    
    Args:
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®
    
    Returns:
        LLMClientLiteå®ä¾‹
        
    Note:
        - è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œå¦‚æœé…ç½®æ–‡ä»¶è¢«ä¿®æ”¹ï¼Œä¼šè‡ªåŠ¨é‡æ–°åŠ è½½
        - ä¹Ÿå¯ä»¥é€šè¿‡ force_reload=True å¼ºåˆ¶é‡æ–°åŠ è½½
    """
    global _client_instance, _config_file_path, _config_file_mtime
    
    # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
    if _config_file_path is None:
        current_dir = Path(__file__).parent
        _config_file_path = str(current_dir.parent / "config" / "run_env_config" / "llm_config.yaml")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(_config_file_path):
        if _client_instance is None:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {_config_file_path}")
        # é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ä½†å·²æœ‰å®ä¾‹ï¼Œè¿”å›ç°æœ‰å®ä¾‹
        return _client_instance
    
    # è·å–å½“å‰é…ç½®æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
    current_mtime = os.path.getmtime(_config_file_path)
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
    need_reload = (
        force_reload or 
        _client_instance is None or 
        _config_file_mtime is None or 
        current_mtime != _config_file_mtime
    )
    
    if need_reload:
        if _config_file_mtime is not None and current_mtime != _config_file_mtime:
            print(f"ğŸ”„ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶å˜åŒ–ï¼Œé‡æ–°åŠ è½½é…ç½®...")
        
        _client_instance = LLMClientLite()
        _config_file_mtime = current_mtime
    
    return _client_instance


def reload_llm_client() -> LLMClientLite:
    """
    å¼ºåˆ¶é‡æ–°åŠ è½½LLMå®¢æˆ·ç«¯é…ç½®
    
    Returns:
        é‡æ–°åŠ è½½åçš„LLMClientLiteå®ä¾‹
    """
    return get_llm_client(force_reload=True)


if __name__ == "__main__":
    # æµ‹è¯•LLMå®¢æˆ·ç«¯
    try:
        client = get_llm_client()
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print(f"   å¯ç”¨æ¨¡å‹: {client.models}")
        print(f"   Base URL: {client.base_url}")
        
        # æµ‹è¯•Visionè°ƒç”¨ï¼ˆéœ€è¦æä¾›çœŸå®çš„å›¾ç‰‡è·¯å¾„ï¼‰
        # result = client.vision_query("/path/to/image.jpg", "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ")
        # print(f"âœ… Visionå“åº”: {result}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

